{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuChat.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPNRYaf/1AiOJ+I4BRu1TTE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuReformer/blob/main/TraxRuChat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSGLCkQTNTyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f14502-e572-4e76-9ba7-0cb598437569"
      },
      "source": [
        "!pip -q install trax==1.3.6"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▊                               | 10kB 23.5MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 19.5MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 14.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 40kB 13.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 51kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 71kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 81kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 92kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 102kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 112kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 122kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 133kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 143kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 153kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 163kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 174kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 184kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 194kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 204kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 215kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 225kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 235kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 245kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 256kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 266kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 276kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 286kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 296kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 307kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 317kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 327kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 337kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 348kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 358kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 368kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 378kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 389kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 399kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 409kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 419kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 430kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 440kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 450kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 460kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 471kB 8.6MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcB4I98_rlXx",
        "outputId": "cada4510-cacb-4a10-eedf-c71e65bcfc5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !pip install --upgrade jaxlib\r\n",
        "!pip install --upgrade jax jaxlib==0.1.61+cuda110 -f https://storage.googleapis.com/jax-releases/jax_releases.html"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_releases.html\n",
            "Collecting jax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/9d/2862825b5eddd0df64c78b22cc0b897f0128b1c6494bf39e4849e9e0fade/jax-0.2.10.tar.gz (589kB)\n",
            "\u001b[K     |████████████████████████████████| 593kB 7.3MB/s \n",
            "\u001b[?25hCollecting jaxlib==0.1.61+cuda110\n",
            "\u001b[?25l  Downloading https://storage.googleapis.com/jax-releases/cuda110/jaxlib-0.1.61%2Bcuda110-cp37-none-manylinux2010_x86_64.whl (155.7MB)\n",
            "\u001b[K     |████████████████████████████████| 155.7MB 85kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: opt_einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib==0.1.61+cuda110) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib==0.1.61+cuda110) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.2.10-cp37-none-any.whl size=679776 sha256=b9a925161d76d487dd7a994e6b967e8e248016d9e69f3427bf019e56c24e039d\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/ea/ac/3be3bc19ee3b62f6fe1561eb6df1199284bb6bab819c1befa4\n",
            "Successfully built jax\n",
            "Installing collected packages: jax, jaxlib\n",
            "  Found existing installation: jax 0.2.9\n",
            "    Uninstalling jax-0.2.9:\n",
            "      Successfully uninstalled jax-0.2.9\n",
            "  Found existing installation: jaxlib 0.1.60+cuda101\n",
            "    Uninstalling jaxlib-0.1.60+cuda101:\n",
            "      Successfully uninstalled jaxlib-0.1.60+cuda101\n",
            "Successfully installed jax-0.2.10 jaxlib-0.1.61+cuda110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv9KUWU4XJVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6e2c56-772e-411d-d9da-f71cf5976718"
      },
      "source": [
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.7                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b4EXajWOAgb",
        "outputId": "ba588f04-2de0-49ae-9433-f5925469a66f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "import re\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "from trax.supervised import training\r\n",
        "\r\n",
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c118cbd117f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# import sentencepiece as spm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtrax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\"\"\"Trax top level import.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfastmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/data/tf_inputs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mt5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt5_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mt5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessors\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt5_processors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mt5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentencepiece_vocabulary\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt5_spc_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mt5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt5_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m   \u001b[0;31m# pylint: disable=g-explicit-tensorflow-version-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'sentencepiece_vocabulary' from 't5.data' (/usr/local/lib/python3.7/dist-packages/t5/data/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc17KUPsOBOF",
        "outputId": "08815e63-ae4c-4904-8e3d-eb4542424c09"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJm2plDAwmLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c803c1-6d01-49a0-f468-555c297d9cca"
      },
      "source": [
        "VOCAB_FILE = 'bpe32.model'\r\n",
        "VOCAB_DIR = '/content/drive/MyDrive/'\r\n",
        "VOCAB_TYPE = 'sentencepiece'\r\n",
        "\r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type= VOCAB_TYPE,\r\n",
        "    vocab_file= VOCAB_FILE,\r\n",
        "    vocab_dir= VOCAB_DIR)\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  32000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJj2ELaDON3u"
      },
      "source": [
        "# Load and clear the dialogues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "PpWXsgzBOVMt",
        "outputId": "82524be2-842f-46bf-bb19-b383b8ebd10d"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/dialogues.tsv', sep='\\t')\r\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>persona_1_profile</th>\n",
              "      <th>persona_2_profile</th>\n",
              "      <th>dialogue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;span class=participant_1&gt;У меня любимая работ...</td>\n",
              "      <td>&lt;span class=participant_2&gt;Ищу принца.&lt;br /&gt;Вед...</td>\n",
              "      <td>&lt;span class=participant_2&gt;Пользователь 2: Прив...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;span class=participant_1&gt;Я работаю учителем&lt;b...</td>\n",
              "      <td>&lt;span class=participant_2&gt;Я бизнесмен&lt;br /&gt;У м...</td>\n",
              "      <td>&lt;span class=participant_1&gt;Пользователь 1: Прив...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;span class=participant_1&gt;Я купила дом&lt;br /&gt;Я ...</td>\n",
              "      <td>&lt;span class=participant_2&gt;Я пою в караоке&lt;br /...</td>\n",
              "      <td>&lt;span class=participant_1&gt;Пользователь 1: Прив...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;span class=participant_1&gt;я врач и женат&lt;br /&gt;...</td>\n",
              "      <td>&lt;span class=participant_2&gt;Я мальчик&lt;br /&gt;Я учу...</td>\n",
              "      <td>&lt;span class=participant_2&gt;Пользователь 2: Здра...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;span class=participant_1&gt;Я школьница.&lt;br /&gt;Я ...</td>\n",
              "      <td>&lt;span class=participant_2&gt;Я простоват.&lt;br /&gt;Лю...</td>\n",
              "      <td>&lt;span class=participant_1&gt;Пользователь 1: Прив...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   persona_1_profile  ...                                           dialogue\n",
              "0  <span class=participant_1>У меня любимая работ...  ...  <span class=participant_2>Пользователь 2: Прив...\n",
              "1  <span class=participant_1>Я работаю учителем<b...  ...  <span class=participant_1>Пользователь 1: Прив...\n",
              "2  <span class=participant_1>Я купила дом<br />Я ...  ...  <span class=participant_1>Пользователь 1: Прив...\n",
              "3  <span class=participant_1>я врач и женат<br />...  ...  <span class=participant_2>Пользователь 2: Здра...\n",
              "4  <span class=participant_1>Я школьница.<br />Я ...  ...  <span class=participant_1>Пользователь 1: Прив...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSuV5mxHOtti",
        "outputId": "92784dff-5ee5-4312-fc89-bd2cf24814a5"
      },
      "source": [
        "test = data.dialogue.iloc[0]\r\n",
        "print(wrapper.fill(test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<span class=participant_2>Пользователь 2: Привет) расскажи о\n",
            "себе</span><br /><span class=participant_1>Пользователь 1: Привет) под\n",
            "вкусный кофеек настроение поболтать появилось<br />)</span><br /><span\n",
            "class=participant_2>Пользователь 2: Что читаешь? Мне нравится\n",
            "классика</span><br /><span class=participant_2>Пользователь 2: Я тоже\n",
            "люблю пообщаться</span><br /><span class=participant_1>Пользователь 1:\n",
            "Люблю животных, просто обожаю, как и свою работу)</span><br /><span\n",
            "class=participant_1>Пользователь 1: Я фантастику люблю</span><br\n",
            "/><span class=participant_2>Пользователь 2: А я выращиваю\n",
            "фиалки</span><br /><span class=participant_2>Пользователь 2: И веду\n",
            "здоровый и активный образ жизни!</span><br /><span\n",
            "class=participant_1>Пользователь 1: Ух ты, интересно.</span><br\n",
            "/><span class=participant_2>Пользователь 2: Ты случайно не принц на\n",
            "белом коне? Я его очень жду<br />..</span><br /><span\n",
            "class=participant_1>Пользователь 1: А у меня из хобби каждую неделю\n",
            "тусить с моим лучшим<br />другом)</span><br />\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK2nRR9WOtwt"
      },
      "source": [
        "def dialogue_clearing(string):\r\n",
        "    '''\r\n",
        "    Clear text from special characters\r\n",
        "    Args:\r\n",
        "        string (string): one dialog text\r\n",
        "    \r\n",
        "    Return\r\n",
        "        list of senteces\r\n",
        "    '''\r\n",
        "    clear_dialogue = []\r\n",
        "    for s in string.split('</span>'):\r\n",
        "        s = re.sub('<span class=participant_1>', '', s)\r\n",
        "        s = re.sub('<span class=participant_2>', '', s)\r\n",
        "        s = re.sub(r'^<br />', '', s)\r\n",
        "        s = re.sub('<br />', ' ', s)\r\n",
        "        s = re.sub(' ,', ',', s)\r\n",
        "        s = re.sub(' \\.', '.', s)\r\n",
        "        s = re.sub(',', ', ', s)\r\n",
        "        s = re.sub('  ', ' ', s)\r\n",
        "        s = re.sub(r'\\r\\n', ' ', s)\r\n",
        "        s = re.sub('\"', '', s)\r\n",
        "        s = re.sub('Пользователь ', '', s)\r\n",
        "        s = re.sub('Пользователь ', '', s)\r\n",
        "        \r\n",
        "        if len(s) != 0:\r\n",
        "            clear_dialogue.append(s)\r\n",
        "    \r\n",
        "    return clear_dialogue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lvWTxGeOt01",
        "outputId": "773b92e7-405e-4870-d50b-acaa6a5a09ec"
      },
      "source": [
        "for s in dialogue_clearing(test):\r\n",
        "    print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пользователь 2: Привет) расскажи о себе\n",
            "Пользователь 1: Привет) под вкусный кофеек настроение поболтать появилось )\n",
            "Пользователь 2: Что читаешь? Мне нравится классика\n",
            "Пользователь 2: Я тоже люблю пообщаться\n",
            "Пользователь 1: Люблю животных, просто обожаю, как и свою работу)\n",
            "Пользователь 1: Я фантастику люблю\n",
            "Пользователь 2: А я выращиваю фиалки\n",
            "Пользователь 2: И веду здоровый и активный образ жизни!\n",
            "Пользователь 1: Ух ты, интересно.\n",
            "Пользователь 2: Ты случайно не принц на белом коне? Я его очень жду..\n",
            "Пользователь 1: А у меня из хобби каждую неделю тусить с моим лучшим другом)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMDGkSU8Ot3-"
      },
      "source": [
        "def dialogue_concat(sentences_list):\r\n",
        "    '''\r\n",
        "    Concatenate several consecutive sentences of one user to one sequence\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        sentences_list (list): list of sentences for one dialogue\r\n",
        "        \r\n",
        "    Return:\r\n",
        "        list of senteces\r\n",
        "    '''\r\n",
        "\r\n",
        "    clear_dialogue_concat = [sentences_list[0]]\r\n",
        "    for i in range(1, len(sentences_list)):\r\n",
        "        s = sentences_list[i]\r\n",
        "        if clear_dialogue_concat[-1][:2] == s[:2]:\r\n",
        "            clear_dialogue_concat[-1] = clear_dialogue_concat[-1] + ' ' + s[3:] \r\n",
        "        else:\r\n",
        "            clear_dialogue_concat.append(s)\r\n",
        "            \r\n",
        "    return clear_dialogue_concat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqR19YiUOt7f",
        "outputId": "01e19e8c-08af-4de3-851f-8a7d1ef46e8c"
      },
      "source": [
        "dialogue = dialogue_clearing(test)\r\n",
        "for s in dialogue_concat(dialogue):\r\n",
        "    print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пользователь 2: Привет) расскажи о себе\n",
            "Пользователь 1: Привет) под вкусный кофеек настроение поболтать появилось )\n",
            "Пользователь 2: Что читаешь? Мне нравится классика Я тоже люблю пообщаться\n",
            "Пользователь 1: Люблю животных, просто обожаю, как и свою работу) Я фантастику люблю\n",
            "Пользователь 2: А я выращиваю фиалки И веду здоровый и активный образ жизни!\n",
            "Пользователь 1: Ух ты, интересно.\n",
            "Пользователь 2: Ты случайно не принц на белом коне? Я его очень жду..\n",
            "Пользователь 1: А у меня из хобби каждую неделю тусить с моим лучшим другом)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f36K9575PjYe"
      },
      "source": [
        "def swap_users(sentence_list):\r\n",
        "    '''\r\n",
        "    Swap user1 and user2\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        sentences_list (list): list of sentences for one dialogue\r\n",
        "        \r\n",
        "    Return:\r\n",
        "        list of senteces\r\n",
        "    '''\r\n",
        "\r\n",
        "    dialogue = []\r\n",
        "    for s in sentence_list:\r\n",
        "        if s.find('2:') == 0:\r\n",
        "            s = re.sub('2:', '1:', s)\r\n",
        "            dialogue.append(s)\r\n",
        "            continue\r\n",
        "    \r\n",
        "        if s.find('1:') == 0:\r\n",
        "            s = re.sub('1:', '2:', s)\r\n",
        "            dialogue.append(s)\r\n",
        "            continue\r\n",
        "    \r\n",
        "    return dialogue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_I35SjOPu6-",
        "outputId": "6cf30951-3120-4665-f169-4bc991f57f71"
      },
      "source": [
        "dialogue = dialogue_clearing(test)\r\n",
        "dialogue = dialogue_concat(dialogue)\r\n",
        "for s in swap_users(dialogue):\r\n",
        "    print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пользователь 1: Привет) расскажи о себе\n",
            "Пользователь 2: Привет) под вкусный кофеек настроение поболтать появилось )\n",
            "Пользователь 1: Что читаешь? Мне нравится классика Я тоже люблю пообщаться\n",
            "Пользователь 2: Люблю животных, просто обожаю, как и свою работу) Я фантастику люблю\n",
            "Пользователь 1: А я выращиваю фиалки И веду здоровый и активный образ жизни!\n",
            "Пользователь 2: Ух ты, интересно.\n",
            "Пользователь 1: Ты случайно не принц на белом коне? Я его очень жду..\n",
            "Пользователь 2: А у меня из хобби каждую неделю тусить с моим лучшим другом)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8OGW4v3P0Ca",
        "outputId": "ffffce53-5ec3-4994-de9b-30251631eff3"
      },
      "source": [
        "untokenized_data = []\r\n",
        "dialogue_len = []\r\n",
        "tmp = []\r\n",
        "for i in tqdm(range(data.shape[0])):\r\n",
        "    dialogue = data.dialogue.iloc[i]\r\n",
        "    \r\n",
        "    dialogue = dialogue_clearing(dialogue)\r\n",
        "    dialogue = dialogue_concat(dialogue)\r\n",
        "    \r\n",
        "    if dialogue[0].find('2:') == 0:\r\n",
        "        dialogue = swap_users(dialogue)\r\n",
        "    \r\n",
        "    tmp.append(dialogue)\r\n",
        "    \r\n",
        "    # augmentation\r\n",
        "    if (len(dialogue[2:]) >= 8): tmp.append(dialogue[2:])\r\n",
        "    if (len(dialogue[4:]) >= 8): tmp.append(dialogue[4:])\r\n",
        "    if (len(dialogue[6:]) >= 8): tmp.append(dialogue[6:])\r\n",
        "    if (len(dialogue[8:]) >= 8): tmp.append(dialogue[8:])\r\n",
        "    if (len(dialogue[10:]) >= 8): tmp.append(dialogue[10:])\r\n",
        "    if (len(dialogue[12:]) >= 8): tmp.append(dialogue[12:])\r\n",
        "    if (len(dialogue[14:]) >= 8): tmp.append(dialogue[14:])\r\n",
        "    if (len(dialogue[16:]) >= 8): tmp.append(dialogue[16:])    \r\n",
        "        \r\n",
        "    for d in tmp:    \r\n",
        "        string = ' '.join(d)\r\n",
        "        string = string.strip()\r\n",
        "        string = string + ' STOP\\n'\r\n",
        "        dialogue_len.append(len(d))\r\n",
        "        untokenized_data.append(string)\r\n",
        "        \r\n",
        "    tmp.clear()\r\n",
        "    \r\n",
        "    random.Random(13).shuffle(untokenized_data)\r\n",
        "    \r\n",
        "with open('dialogues2.txt', 'w', encoding='utf-8') as file:\r\n",
        "     file.writelines(untokenized_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10013/10013 [00:04<00:00, 2358.29it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mRW_ERJQFeO",
        "outputId": "3968392b-9c52-4d40-8131-9180a66c38dd"
      },
      "source": [
        "print(wrapper.fill(untokenized_data[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Пользователь 1: Привет) расскажи о себе Пользователь 2: Привет) под\n",
            "вкусный кофеек настроение поболтать появилось ) Пользователь 1: Что\n",
            "читаешь? Мне нравится классика Я тоже люблю пообщаться Пользователь 2:\n",
            "Люблю животных, просто обожаю, как и свою работу) Я фантастику люблю\n",
            "Пользователь 1: А я выращиваю фиалки И веду здоровый и активный образ\n",
            "жизни! Пользователь 2: Ух ты, интересно. Пользователь 1: Ты случайно\n",
            "не принц на белом коне? Я его очень жду.. Пользователь 2: А у меня из\n",
            "хобби каждую неделю тусить с моим лучшим другом)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcD5sF7rQg17"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=dialogues2.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                 --model_prefix=bpe --vocab_size=32000 --model_type=bpe')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WlMwLaMQSgO"
      },
      "source": [
        "# Data preprocessing and create the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3NK5Hw7QJln",
        "outputId": "f6c147e4-6999-4841-fcbe-1960048c0e52"
      },
      "source": [
        "# loading prepared set to save time\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/dialogues2.txt', 'r', encoding='utf-8') as file:\r\n",
        "     untokenized_data = file.readlines() \r\n",
        "\r\n",
        "untokenized_data = [s.strip() for s in untokenized_data]\r\n",
        "\r\n",
        "print(wrapper.fill(untokenized_data[0]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1: А тебя как зовут? 2: Виктор 1: Приятно познакомиться) 2: Чем\n",
            "увлекаешься? 1: Кем работаешь? Чем увлекаешься? 2: Я таксист и мне\n",
            "приходится работать по ночам А чем увлекаешься ты? 1: Я спортсмен-\n",
            "плавец, люблю работать, как над собой, так и в целом А увлечение,\n",
            "очень вкусно готовлю У тебя есть домашнее животные? 2: Ты большой\n",
            "молодец, кстати, я увлекаюсь теннисом, как тебе такой вид спорта? Нет,\n",
            "домашних животных нет 1: Плаванье) Я очень хочу хомяка, а жена не\n",
            "разрешает Какое кино нравиться? 2: Эээх, очень жаль, ну ничего, может\n",
            "когда- нибудь ты исполнить свою мечту 1: Хотелось бы) 2: Кино не\n",
            "люблю, я больше музыку слушаю, в особенности шансон 1: У тебя есть\n",
            "мечта? А я постоянно хожу в кинотеатры на приметные) 2: Да, конечно, а\n",
            "у тебя? Есть какой-то любимый жанр? Кстати, я еще люблю читать А ты\n",
            "любишь книги? 1: Да, мистика Когда все на гране) А какие книги любишь?\n",
            "2: Ооо, интересно звучит 1: Тоже, всего Булгакова перечитал 2:\n",
            "Детективы Ооо Здорово 1: Убийство в полярном экспресс читал? 2: Нет А\n",
            "ты? 1: Очень крутая книга Рекомендую) 2: Возьму на заметку 1: Приятно\n",
            "было пообщаться 2: И мне Пока STOP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHPIJPH6E3dT"
      },
      "source": [
        "# Attention! do not run\r\n",
        "# random.shuffle(untokenized_data)\r\n",
        "# print(wrapper.fill(untokenized_data[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqxEykiLRhoo",
        "outputId": "10571b39-a518-456e-f290-b5494644aaf4"
      },
      "source": [
        "# train/eval split\r\n",
        "margin = int(len(untokenized_data)*0.95)\r\n",
        "\r\n",
        "train_dialogues = untokenized_data[:margin]\r\n",
        "print('train cases: ', len(train_dialogues))\r\n",
        "eval_dialogues = untokenized_data[margin:]\r\n",
        "print('eval cases: ', len(eval_dialogues))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  44460\n",
            "eval cases:  2341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rlD2rxFQx6A"
      },
      "source": [
        "# clear mem\r\n",
        "# del untokenized_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W724qoS9R783"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list of dialogues\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        random.shuffle(index_list) # re-shuffle the order\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample, sample) # input and target\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_dialogues, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_dialogues, shuffle=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE0jmPywSPbI"
      },
      "source": [
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type=VOCAB_TYPE,\r\n",
        "        vocab_file=VOCAB_FILE,\r\n",
        "        vocab_dir=VOCAB_DIR) # loading pre-prepared model to save time\r\n",
        "    return s\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type=VOCAB_TYPE,\r\n",
        "        vocab_file=VOCAB_FILE,\r\n",
        "        vocab_dir=VOCAB_DIR))\r\n",
        "    \r\n",
        "    return list(inputs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHOE73afSunn"
      },
      "source": [
        "# trax allows us to use combinators to generate our data pipeline\r\n",
        "data_pipeline = trax.data.Serial(\r\n",
        "    \r\n",
        "    # tokenize the data\r\n",
        "    trax.data.Tokenize(vocab_type=VOCAB_TYPE,\r\n",
        "                       vocab_dir=VOCAB_DIR,\r\n",
        "                       vocab_file=VOCAB_FILE),\r\n",
        "    \r\n",
        "    # filter too long sequences\r\n",
        "    trax.data.FilterByLength(max_length=2048, min_length=64),\r\n",
        "    \r\n",
        "    # bucket by length\r\n",
        "    trax.data.BucketByLength(boundaries=[128, 256, 512, 1024],\r\n",
        "                             batch_sizes=[16, 8, 4, 2, 1]),\r\n",
        "    \r\n",
        "    # add loss weights but do not add it to the padding tokens (i.e. 0)\r\n",
        "    trax.data.AddLossWeights(id_to_mask=0)\r\n",
        ")\r\n",
        "\r\n",
        "# apply the data pipeline to our train and eval sets\r\n",
        "train_stream = data_pipeline(data_generator(train_dialogues))\r\n",
        "eval_stream = data_pipeline(data_generator(eval_dialogues))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlZrmKpkUrYR",
        "outputId": "cdf9fe97-8b68-4dae-a25a-46c68f979eca"
      },
      "source": [
        "# the stream generators will yield (input, target, weights). let's just grab the input for inspection\r\n",
        "inp, _, m = next(train_stream)\r\n",
        "\r\n",
        "# print the shape. format is (batch size, token length)\r\n",
        "print(\"input shape: \", inp.shape)\r\n",
        "\r\n",
        "# detokenize the first element\r\n",
        "print(detokenize(inp[0]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape:  (8, 256)\n",
            "1: Надоела зима, скорее бы лето. Люблю его очень 2: Вы работаете? 1: Скорее всего что я работаю Как в же в городе в котором я живу не работать то ? 😊 Ася, вы были в горах? Я вот никогда не была но очень хочу в Тибет, либо на Гималаи, но на худой конец и уральский хребет сойдет 😊 А чем вы занимаетесь? 2: Я медсестра и стендап-комик, люблю животных А вы? У меня 4 собаки 1: Я очень люблю кошек, и у меня одна есть Не много ли собак у вас? 2: Думаю, что нет. Вы замужем? 1: Нет не замужем, у меня же кот есть😀 А ещё я люблю фрукты 2: А я вот вышла замуж за друга)А вы юмористка. 1: Особенно манго и апельсины 2: Любите готовить? 1: Терпеть не могу готовить А юморить моё хобби😊 STOP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQr1gruD3XQ2",
        "outputId": "2bb3d882-a1d9-4571-cc9e-59068df418af"
      },
      "source": [
        "m[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZz6hVaDfvOJ"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSkIwZxDfdux"
      },
      "source": [
        "def ReformerLM(vocab_size=32000, n_layers=2, mode='train', attention_type=tl.SelfAttention):\r\n",
        "    \"\"\"\r\n",
        "    Args: \r\n",
        "        vocab_size (int): size of the vocabulary\r\n",
        "        n_layers (int): number of decoder layers\r\n",
        "        mode (string): setting of the model which can be 'train', 'eval', or 'predict' \r\n",
        "        attention_type(class): attention class to use \r\n",
        "    Returns: \r\n",
        "        model (TransformerLM): a transformer language model implemented in Trax\r\n",
        "    \"\"\"    \r\n",
        "    # initialize an instance of Trax's TransformerLM class\r\n",
        "    model = trax.models.ReformerLM( \r\n",
        "        vocab_size=vocab_size,\r\n",
        "        # d_model=d_model,\r\n",
        "        # d_ff=d_ff,\r\n",
        "        n_layers=n_layers,\r\n",
        "        # max_len=max_len,\r\n",
        "        # dropout = dropout,\r\n",
        "        mode = mode,\r\n",
        "        attention_type=attention_type\r\n",
        "    )\r\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bykCKvza2Yw",
        "outputId": "48c65db6-6e73-43fe-e77d-67b231158191",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tmp_model = ReformerLM(n_layers=2, mode='train', attention_type=tl.SelfAttention)\r\n",
        "tmp_model.sublayers[-1] = tl.Serial(tl.Dense(vocab_size), tl.LogSoftmax())\r\n",
        "print(tmp_model)\r\n",
        "del tmp_model"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_32000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Dup_out2\n",
            "  ReversibleSerial_in2_out2[\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "  ]\n",
            "  Concatenate_in2\n",
            "  LayerNorm\n",
            "  Dropout\n",
            "  Serial[\n",
            "    Dense_32000\n",
            "    LogSoftmax\n",
            "  ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th5ZANZZRi4X"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxIxaaotbqLN"
      },
      "source": [
        "ChatModel = ReformerLM(vocab_size=32000, n_layers=6, mode='train')\r\n",
        "ChatModel.sublayers[-1] = tl.Serial(tl.Dense(vocab_size), tl.LogSoftmax())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGDjwzKgRhkn"
      },
      "source": [
        "def training_loop(ChatModel, train_gen, eval_gen, output_dir = \"~/model/\"):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "        ChatModel:  the Reformer language model with LogSoftmax output\r\n",
        "        train_gen (generator): train data generator.\r\n",
        "        eval_gen (generator): Validation generator. \r\n",
        "        output_dir (string): Path to save the model output. Defaults to './model/'.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop for the model.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    output_dir = os.path.expanduser(output_dir)\r\n",
        "\r\n",
        "    # use the warmup_and_rsqrt_decay learning rate schedule\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=4000, max_value=0.0002)\r\n",
        "\r\n",
        "    train_task = training.TrainTask(            \r\n",
        "        labeled_data=train_gen,\r\n",
        "        loss_layer=tl.CrossEntropyLoss(),\r\n",
        "        optimizer=trax.optimizers.Adam(2e-4),\r\n",
        "        # lr_schedule=lr_schedule,\r\n",
        "        n_steps_per_checkpoint=10\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask(                      \r\n",
        "        labeled_data=eval_gen,\r\n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]\r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(ChatModel,\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    return loop"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fml0UGRASDoM"
      },
      "source": [
        "!cp /content/drive/MyDrive/model2/model_6_8_02_100.pkl.gz ~/model/model.pkl.gz"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t67oYGO5sxK1"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU1tQpq0sya6"
      },
      "source": [
        "%tensorboard --logdir ../root/model/ # train = name of folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gX2SUFARd3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbd6a42-6f0f-4ef9-85ac-e9a3147ce629"
      },
      "source": [
        "# !rm -f model/model.pkl.gz\r\n",
        "loop = training_loop(ChatModel, train_stream, eval_stream)\r\n",
        "loop.run(100)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model2/rmodel_test.pkl.gz"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 69136640\n",
            "Step      1: Ran 1 train steps in 124.59 secs\n",
            "Step      1: train CrossEntropyLoss |  10.40490532\n",
            "Step      1: eval  CrossEntropyLoss |  10.19735813\n",
            "Step      1: eval          Accuracy |  0.00213068\n",
            "\n",
            "Step     10: Ran 9 train steps in 370.62 secs\n",
            "Step     10: train CrossEntropyLoss |  9.49746132\n",
            "Step     10: eval  CrossEntropyLoss |  8.87853241\n",
            "Step     10: eval          Accuracy |  0.05470914\n",
            "\n",
            "Step     20: Ran 10 train steps in 158.22 secs\n",
            "Step     20: train CrossEntropyLoss |  8.10419369\n",
            "Step     20: eval  CrossEntropyLoss |  7.84483194\n",
            "Step     20: eval          Accuracy |  0.04067197\n",
            "\n",
            "Step     30: Ran 10 train steps in 160.80 secs\n",
            "Step     30: train CrossEntropyLoss |  7.36917973\n",
            "Step     30: eval  CrossEntropyLoss |  7.04085541\n",
            "Step     30: eval          Accuracy |  0.04953999\n",
            "\n",
            "Step     40: Ran 10 train steps in 159.63 secs\n",
            "Step     40: train CrossEntropyLoss |  7.06333065\n",
            "Step     40: eval  CrossEntropyLoss |  7.10729074\n",
            "Step     40: eval          Accuracy |  0.05211480\n",
            "\n",
            "Step     50: Ran 10 train steps in 157.52 secs\n",
            "Step     50: train CrossEntropyLoss |  6.98024750\n",
            "Step     50: eval  CrossEntropyLoss |  6.90197611\n",
            "Step     50: eval          Accuracy |  0.04192355\n",
            "\n",
            "Step     60: Ran 10 train steps in 159.19 secs\n",
            "Step     60: train CrossEntropyLoss |  7.04194784\n",
            "Step     60: eval  CrossEntropyLoss |  7.21898556\n",
            "Step     60: eval          Accuracy |  0.05259313\n",
            "\n",
            "Step     70: Ran 10 train steps in 159.38 secs\n",
            "Step     70: train CrossEntropyLoss |  7.04204178\n",
            "Step     70: eval  CrossEntropyLoss |  6.85829401\n",
            "Step     70: eval          Accuracy |  0.05903790\n",
            "\n",
            "Step     80: Ran 10 train steps in 158.89 secs\n",
            "Step     80: train CrossEntropyLoss |  7.02523947\n",
            "Step     80: eval  CrossEntropyLoss |  7.50239658\n",
            "Step     80: eval          Accuracy |  0.04988488\n",
            "\n",
            "Step     90: Ran 10 train steps in 159.04 secs\n",
            "Step     90: train CrossEntropyLoss |  7.06895685\n",
            "Step     90: eval  CrossEntropyLoss |  6.91415501\n",
            "Step     90: eval          Accuracy |  0.04988831\n",
            "\n",
            "Step    100: Ran 10 train steps in 160.14 secs\n",
            "Step    100: train CrossEntropyLoss |  7.07383633\n",
            "Step    100: eval  CrossEntropyLoss |  7.07785559\n",
            "Step    100: eval          Accuracy |  0.04872007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNILzgz6Ls8F"
      },
      "source": [
        "# !cp ~/model/model.pkl.gz /content/drive/MyDrive/model2/\r\n",
        "# !cp ~/model/model.pkl.gz /content/drive/MyDrive/model2/model_6_8_02_20.pkl.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8rd7b7oKK12"
      },
      "source": [
        "## GPU memory footprint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx3fiEFMtm7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31f1db09-a6f6-4a2c-d79a-9131da94612b"
      },
      "source": [
        "# memory footprint support libraries/code\r\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\r\n",
        "!pip install gputil\r\n",
        "!pip install psutil\r\n",
        "!pip install humanize\r\n",
        "import psutil\r\n",
        "import humanize\r\n",
        "import os\r\n",
        "import GPUtil as GPU\r\n",
        "GPUs = GPU.getGPUs()\r\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\r\n",
        "gpu = GPUs[0]\r\n",
        "def printm():\r\n",
        " process = psutil.Process(os.getpid())\r\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\r\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\r\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=fdef2c61bbcead285449bcced893547d3c20f505352fb175df3fa432fc633ea8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Gen RAM Free: 10.7 GB  | Proc size: 4.2 GB\n",
            "GPU RAM Free: 568MB | Used: 7043MB | Util  93% | Total 7611MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxsr9DDYiKk"
      },
      "source": [
        "# Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAgq5k-d267N"
      },
      "source": [
        "## Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg6247VCucGY"
      },
      "source": [
        "!cp /content/drive/MyDrive/model2/model.pkl.gz ~/model/model.pkl.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKiyx7aSwjha"
      },
      "source": [
        "# define the `predict_mem_len` and `predict_drop_len` of tl.SelfAttention\r\n",
        "def attention(*args, **kwargs):\r\n",
        "    # number of input positions to remember in a cache when doing fast inference. \r\n",
        "    kwargs['predict_mem_len'] = 120\r\n",
        "    # number of input elements to drop once the fast inference input cache fills up.\r\n",
        "    kwargs['predict_drop_len'] = 120\r\n",
        "    # return the attention layer with the parameters defined above\r\n",
        "    return tl.SelfAttention(*args, **kwargs)\r\n",
        "\r\n",
        "# define the model using the ReformerLM function you implemented earlier.\r\n",
        "ChatModel = ReformerLM(\r\n",
        "    vocab_size=32000,\r\n",
        "    n_layers=6,\r\n",
        "    mode='predict',\r\n",
        "    attention_type=attention,\r\n",
        ")\r\n",
        "\r\n",
        "# define an input signature so we can initialize our model. shape will be (1, 1) and the data type is int32.\r\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZDLV6et4Ds"
      },
      "source": [
        "# define the model using the ReformerLM function you implemented earlier.\r\n",
        "# ChatModel = TransformerLM(n_layers=6, mode='predict')\r\n",
        "# ChatModel.sublayers[-1] = tl.Serial(tl.Dense(vocab_size), tl.LogSoftmax())\r\n",
        "\r\n",
        "# define an input signature so we can initialize our model. shape will be (1, 1) and the data type is int32.\r\n",
        "# shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tuxn1_Dt4Nr",
        "outputId": "484efd4d-7c2b-4d18-b3a3-93f31bbf442e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# initialize from file\r\n",
        "ChatModel.init_from_file('../root/model/model.pkl.gz', weights_only=True, input_signature=shape11)\r\n",
        "# ChatModel.init_from_file('/content/drive/MyDrive/model2/model_6_8_02_100.pkl.gz', weights_only=True, input_signature=shape11)\r\n",
        "\r\n",
        "# save the starting state\r\n",
        "STARTING_STATE = ChatModel.state"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LayerError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLayerError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-37111effe97a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initialize from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mChatModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../root/model/model.pkl.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# ChatModel.init_from_file('/content/drive/MyDrive/model2/model_6_8_02_100.pkl.gz', weights_only=True, input_signature=shape11)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# save the starting state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36minit_from_file\u001b[0;34m(self, file_name, weights_only, input_signature)\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, input_signature, rng, use_cache)\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_short_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m       raise LayerError(name, 'init', self._caller,\n\u001b[0;32m--> 307\u001b[0;31m                        input_signature, trace) from None\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minit_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLayerError\u001b[0m: Exception passing through layer Serial (in init):\n  layer created in file [...]/models/reformer/reformer.py, line 185\n  layer input shapes: ShapeDtype{shape:(1, 1), dtype:<class 'numpy.int32'>}\n\n  File [...]/trax/layers/combinators.py, line 105, in init_weights_and_state\n    sublayer.init(inputs, use_cache=True))\n\nLayerError: Exception passing through layer ReversibleSerial (in init):\n  layer created in file [...]/models/reformer/reformer.py, line 179\n  layer input shapes: (ShapeDtype{shape:(1, 1, 512), dtype:float32}, ShapeDtype{shape:(1, 1, 512), dtype:float32})\n\n  File [...]/trax/layers/combinators.py, line 105, in init_weights_and_state\n    sublayer.init(inputs, use_cache=True))\n\nLayerError: Exception passing through layer ReversibleHalfResidual (in init):\n  layer created in file [...]/models/reformer/reformer.py, line 69\n  layer input shapes: (ShapeDtype{shape:(1, 1, 512), dtype:float32}, ShapeDtype{shape:(1, 1, 512), dtype:float32})\n\n  File [...]/trax/layers/reversible.py, line 326, in init_weights_and_state\n    attn_weights, attn_state = self.attention_layer.init(inputs)\n\nLayerError: Exception passing through layer SelfAttention (in init):\n  layer created in file [...]/layers/research/efficient_attention.py, line 1009\n  layer input shapes: ShapeDtype{shape:(1, 1, 512), dtype:float32}\n\n  File [...]/layers/research/efficient_attention.py, line 1072, in init_weights_and_state\n    self.rng, weights_rng, state_rng = fastmath.random.split(self.rng, 3)\n\n  File [...]/trax/fastmath/ops.py, line 71, in split\n    return backend()['random_split'](prng, num)\n\n  File [...]/jax/_src/random.py, line 260, in split\n    return _split(key, int(num))  # type: ignore\n\n  File [...]/jax/_src/traceback_util.py, line 139, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n\n  File [...]/dist-packages/jax/api.py, line 416, in f_jitted\n    return cpp_jitted_f(context, *args, **kwargs)\n\n  File [...]/dist-packages/jax/api.py, line 303, in cache_miss\n    donated_invars=donated_invars)\n\n  File [...]/dist-packages/jax/core.py, line 1394, in bind\n    return call_bind(self, fun, *args, **params)\n\n  File [...]/dist-packages/jax/core.py, line 1379, in call_bind\n    top_trace = find_top_trace(args)\n\n  File [...]/dist-packages/jax/core.py, line 818, in find_top_trace\n    top_tracer._assert_live()\n\n  File [...]/jax/interpreters/partial_eval.py, line 940, in _assert_live\n    raise core.escaped_tracer_error(self, None)\n\njax.core.UnexpectedTracerError: Encountered an unexpected tracer. Perhaps this tracer escaped through global state from a previously traced function.\nThe functions being transformed should not save traced values to global state.\n\nThe tracer that caused this error was created on line /usr/local/lib/python3.7/dist-packages/trax/layers/research/efficient_attention.py:1072 (init_weights_and_state).\nWhen the tracer was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n\n/usr/local/lib/python3.7/dist-packages/trax/layers/combinators.py:105 (init_weights_and_state)\n/usr/local/lib/python3.7/dist-packages/trax/layers/base.py:294 (init)\n\n/usr/local/lib/python3.7/dist-packages/trax/layers/reversible.py:326 (init_weights_and_state)\n/usr/local/lib/python3.7/dist-packages/trax/layers/base.py:294 (init)\n\n/usr/local/lib/python3.7/dist-packages/trax/layers/research/efficient_attention.py:1072 (init_weights_and_state)\nThe function being traced when the tracer leaked was call_wrapped at /usr/local/lib/python3.7/dist-packages/jax/linear_util.py:152.\n\nTo catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX5oCsq-dC_7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdnmqmftdLk-"
      },
      "source": [
        "## Full Text Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ7aZKaRt4U8"
      },
      "source": [
        "def output_gen(ChatModel, start_sentence, temperature, accelerate=True):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "        ChatModel:  the Transformer language model\r\n",
        "        start_sentence (string): starting sentence of the conversation\r\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\r\n",
        "            0.0: same as argmax, always pick the most probable token\r\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        generator: yields the next symbol generated by the model\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    input_tokens = tokenize(start_sentence)\r\n",
        "    \r\n",
        "    # Add batch dimension to array\r\n",
        "    input_tokens_with_batch = np.expand_dims(input_tokens, axis=0)\r\n",
        "    \r\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \r\n",
        "        ChatModel,\r\n",
        "        inputs=input_tokens_with_batch,\r\n",
        "        temperature=temperature,\r\n",
        "        accelerate=accelerate\r\n",
        "    )\r\n",
        "\r\n",
        "    return output_gen"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQvs9Hh2voXg"
      },
      "source": [
        "def generate_dialogue(ChatModel, model_state, start_sentence, max_len, temperature, accelerate):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "        ChatModel:  the Transformer language model\r\n",
        "        model_state (np.array): initial state of the model before decoding\r\n",
        "        start_sentence (string): starting sentence of the conversation\r\n",
        "        max_len (int): maximum number of tokens to generate \r\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\r\n",
        "            0.0: same as argmax, always pick the most probable token\r\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        generator: yields the next symbol generated by the model\r\n",
        "    \"\"\"  \r\n",
        "    \r\n",
        "    delimiter_1 = '1:' \r\n",
        "    delimiter_2 = '2:'\r\n",
        "    \r\n",
        "    # initialize detokenized output\r\n",
        "    sentence = ''\r\n",
        "    \r\n",
        "    # token counter\r\n",
        "    counter = 0\r\n",
        "\r\n",
        "    result = []\r\n",
        "    \r\n",
        "    # reset the model state when starting a new dialogue\r\n",
        "    ChatModel.state = model_state\r\n",
        "    \r\n",
        "    # calls the output generator\r\n",
        "    output = output_gen(ChatModel, start_sentence, temperature=temperature, accelerate=accelerate)\r\n",
        "    \r\n",
        "    # print the starting sentence\r\n",
        "    print(start_sentence.split(delimiter_2)[0])\r\n",
        "\r\n",
        "    control = []\r\n",
        "    \r\n",
        "    \r\n",
        "    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.\r\n",
        "    for o in output:\r\n",
        "        \r\n",
        "        result.append(o)\r\n",
        "        control.append(int(o))\r\n",
        "        \r\n",
        "        sentence = detokenize(np.concatenate(result, axis=0))\r\n",
        "\r\n",
        "        if sentence.endswith(delimiter_1):\r\n",
        "            sentence = sentence.split(delimiter_1)[0]\r\n",
        "            print(delimiter_2 + ' ' + sentence)\r\n",
        "            sentence = ''\r\n",
        "            result.clear()\r\n",
        "        \r\n",
        "        elif sentence.endswith(delimiter_2):\r\n",
        "            sentence = sentence.split(delimiter_2)[0]\r\n",
        "            print(delimiter_1 + ' ' + sentence)\r\n",
        "            sentence = ''\r\n",
        "            result.clear()\r\n",
        "\r\n",
        "        counter += 1\r\n",
        "        \r\n",
        "        if counter > max_len:\r\n",
        "            break\r\n",
        "\r\n",
        "        if o[0] == 139:\r\n",
        "            break\r\n",
        "\r\n",
        "\r\n",
        "    return detokenize(control), control"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjJKNcfesBS5"
      },
      "source": [
        "sample_sentence = '1: Привет) 2: Привет!'\r\n",
        "control1, control2 = generate_dialogue(ChatModel, model_state=STARTING_STATE, start_sentence=sample_sentence, max_len=200, temperature=0.0, accelerate=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oO6jRlTBWX2"
      },
      "source": [
        "sample_sentence = '1: Как тебя зовут? 2:'\r\n",
        "_, _ = generate_dialogue(ChatModel, model_state=STARTING_STATE, start_sentence=sample_sentence, max_len=200, temperature=0.2, accelerate=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Op3G2dBWep"
      },
      "source": [
        "sample_sentence = '1: Добрый вечер 2: Доброе утро! 1: Когда увидимся?'\r\n",
        "_, _ = generate_dialogue(ChatModel, model_state=STARTING_STATE, start_sentence=sample_sentence, max_len=200, temperature=0.0, accelerate=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM30ml04bg4x"
      },
      "source": [
        "## Interactive Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X6GvIPRblKF"
      },
      "source": [
        "def next_symbol(model, input_tokens, temperature=0.0):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_tokens (list): tokenized sentence\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "    batch = np.expand_dims(input_tokens, axis=0)\r\n",
        "\r\n",
        "    logits = model(batch)\r\n",
        "    # print(logits.shape)\r\n",
        "    logits = logits[:, -1, :][0]\r\n",
        "    sample = tl.logsoftmax_sample(logits, temperature=temperature)\r\n",
        "    # sample = int(np.argmax(logits))\r\n",
        "    # print(logits[0])\r\n",
        "    \r\n",
        "    return sample"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSyuDiAcyMGL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fc0804ed-8875-4a03-c576-5bf84784ac49"
      },
      "source": [
        "ChatModel.state = STARTING_STATE\r\n",
        "cur_output_tokens = tokenize('1: Добрый вечер 2: Как')\r\n",
        "detokenize([next_symbol(ChatModel, cur_output_tokens)])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'дела'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3aYkPD2bq6c"
      },
      "source": [
        "def interactive_decode_bot_first(model, model_state, greeting='Привет', accelerate=False):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "        model_state: initial state of the model before decoding\r\n",
        "        greeting (string):\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    delimiter_1 = '1:' \r\n",
        "    delimiter_2 = '2:'\r\n",
        "    input_sentence = ''\r\n",
        "\r\n",
        "    cur_output_tokens = tokenize('1: ' + greeting + ' 2:')\r\n",
        "    result = cur_output_tokens.copy()[1:]\r\n",
        "    cur_output = 0\r\n",
        "    stop = 129\r\n",
        "\r\n",
        "    fast_model = tl.Accelerate(model) if accelerate else model\r\n",
        "\r\n",
        "    while (cur_output != stop or input_sentence != 'STOP'):\r\n",
        "\r\n",
        "        # clear past diolog state\r\n",
        "        fast_model.state = model_state\r\n",
        "\r\n",
        "        result.append(cur_output)\r\n",
        "        sentence = detokenize(result)\r\n",
        "\r\n",
        "        # if sentence.endswith(delimiter_1):\r\n",
        "        #     sentence = sentence.split(delimiter_1)[0]\r\n",
        "        #     print(delimiter_2 + ' ' + sentence)\r\n",
        "        #     sentence = ''\r\n",
        "        #     result.clear()\r\n",
        "        \r\n",
        "        if sentence.endswith(delimiter_2):\r\n",
        "            sentence = sentence.split(delimiter_2)[0]\r\n",
        "            print(delimiter_1 + ' ' + sentence)\r\n",
        "            sentence = ''\r\n",
        "            result.clear()\r\n",
        "\r\n",
        "            input_sentence = input('2: ')\r\n",
        "            input_sentence = tokenize(input_sentence)\r\n",
        "            cur_output_tokens += input_sentence + [9]\r\n",
        "            \r\n",
        "\r\n",
        "        cur_output = next_symbol(fast_model, cur_output_tokens)\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "\r\n",
        "    return None"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj0AlOpFbr2Y"
      },
      "source": [
        "interactive_decode_bot_first(ChatModel, STARTING_STATE, greeting='Добрый вечер')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5k2h6Habr7Z"
      },
      "source": [
        "def interactive_decode_bot_second(model, model_state):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "        model_state: initial state of the model before decoding\r\n",
        "\r\n",
        "    Returns: None\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    delimiter_1 = '1:' \r\n",
        "    delimiter_2 = '2:'\r\n",
        "\r\n",
        "    start_sentence = input('1:')\r\n",
        "    cur_output_tokens = tokenize('1: ' + start_sentence + ' 2:')\r\n",
        "    result = cur_output_tokens.copy()[1:]\r\n",
        "    cur_output = 0\r\n",
        "    stop = 129\r\n",
        "\r\n",
        "    while cur_output != stop:\r\n",
        "\r\n",
        "        # clear past diolog state\r\n",
        "        model.state = model_state\r\n",
        "\r\n",
        "        result.append(cur_output)\r\n",
        "        sentence = detokenize(result)\r\n",
        "\r\n",
        "        if sentence.endswith(delimiter_1):\r\n",
        "            sentence = sentence.split(delimiter_1)[0]\r\n",
        "            print(delimiter_2 + ' ' + sentence)\r\n",
        "            sentence = ''\r\n",
        "            result.clear()\r\n",
        "        \r\n",
        "        # if sentence.endswith(delimiter_2):\r\n",
        "        #     sentence = sentence.split(delimiter_2)[0]\r\n",
        "        #     print(delimiter_1 + ' ' + sentence)\r\n",
        "        #     sentence = ''\r\n",
        "        #     result.clear()\r\n",
        "\r\n",
        "            input_sentence = input('1: ')\r\n",
        "            input_sentence = tokenize(input_sentence)\r\n",
        "            cur_output_tokens += input_sentence + [11]\r\n",
        "            \r\n",
        "\r\n",
        "        cur_output = next_symbol(model, cur_output_tokens)\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "\r\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aedRscFAqI5"
      },
      "source": [
        "interactive_decode_bot_second(ChatModel, STARTING_STATE)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}