{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraxRuChat.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPNRYaf/1AiOJ+I4BRu1TTE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khmelkoff/TraxRuReformer/blob/main/TraxRuChat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSGLCkQTNTyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f14502-e572-4e76-9ba7-0cb598437569"
      },
      "source": [
        "!pip -q install trax==1.3.6"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |‚ñä                               | 10kB 23.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñç                              | 20kB 19.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà                              | 30kB 14.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñâ                             | 40kB 13.3MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñå                            | 51kB 9.3MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñè                           | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà                           | 71kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 81kB 10.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 92kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         | 102kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 112kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 122kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 133kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 143kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 153kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 163kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    | 174kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 184kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 194kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 204kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 215kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 225kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 235kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 245kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 256kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 266kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 276kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 286kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé           | 296kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 307kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 317kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 327kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 337kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 348kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 358kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 368kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 378kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 389kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 399kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 409kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 419kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 430kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 440kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 450kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 460kB 8.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 471kB 8.6MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcB4I98_rlXx",
        "outputId": "cada4510-cacb-4a10-eedf-c71e65bcfc5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !pip install --upgrade jaxlib\r\n",
        "!pip install --upgrade jax jaxlib==0.1.61+cuda110 -f https://storage.googleapis.com/jax-releases/jax_releases.html"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_releases.html\n",
            "Collecting jax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/9d/2862825b5eddd0df64c78b22cc0b897f0128b1c6494bf39e4849e9e0fade/jax-0.2.10.tar.gz (589kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 593kB 7.3MB/s \n",
            "\u001b[?25hCollecting jaxlib==0.1.61+cuda110\n",
            "\u001b[?25l  Downloading https://storage.googleapis.com/jax-releases/cuda110/jaxlib-0.1.61%2Bcuda110-cp37-none-manylinux2010_x86_64.whl (155.7MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 155.7MB 85kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: opt_einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib==0.1.61+cuda110) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib==0.1.61+cuda110) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.2.10-cp37-none-any.whl size=679776 sha256=b9a925161d76d487dd7a994e6b967e8e248016d9e69f3427bf019e56c24e039d\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/ea/ac/3be3bc19ee3b62f6fe1561eb6df1199284bb6bab819c1befa4\n",
            "Successfully built jax\n",
            "Installing collected packages: jax, jaxlib\n",
            "  Found existing installation: jax 0.2.9\n",
            "    Uninstalling jax-0.2.9:\n",
            "      Successfully uninstalled jax-0.2.9\n",
            "  Found existing installation: jaxlib 0.1.60+cuda101\n",
            "    Uninstalling jaxlib-0.1.60+cuda101:\n",
            "      Successfully uninstalled jaxlib-0.1.60+cuda101\n",
            "Successfully installed jax-0.2.10 jaxlib-0.1.61+cuda110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv9KUWU4XJVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6e2c56-772e-411d-d9da-f71cf5976718"
      },
      "source": [
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.7                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b4EXajWOAgb",
        "outputId": "ba588f04-2de0-49ae-9433-f5925469a66f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import random\r\n",
        "import re\r\n",
        "# import sentencepiece as spm\r\n",
        "\r\n",
        "import trax\r\n",
        "from trax import layers as tl\r\n",
        "from trax.supervised import decoding\r\n",
        "from trax.supervised import training\r\n",
        "\r\n",
        "import textwrap\r\n",
        "wrapper = textwrap.TextWrapper(width=70)\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c118cbd117f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# import sentencepiece as spm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtrax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\"\"\"Trax top level import.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfastmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/data/tf_inputs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mt5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt5_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mt5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessors\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt5_processors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mt5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentencepiece_vocabulary\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt5_spc_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mt5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt5_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m   \u001b[0;31m# pylint: disable=g-explicit-tensorflow-version-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'sentencepiece_vocabulary' from 't5.data' (/usr/local/lib/python3.7/dist-packages/t5/data/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc17KUPsOBOF",
        "outputId": "08815e63-ae4c-4904-8e3d-eb4542424c09"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJm2plDAwmLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c803c1-6d01-49a0-f468-555c297d9cca"
      },
      "source": [
        "VOCAB_FILE = 'bpe32.model'\r\n",
        "VOCAB_DIR = '/content/drive/MyDrive/'\r\n",
        "VOCAB_TYPE = 'sentencepiece'\r\n",
        "\r\n",
        "vocab_size = trax.data.vocab_size(\r\n",
        "    vocab_type= VOCAB_TYPE,\r\n",
        "    vocab_file= VOCAB_FILE,\r\n",
        "    vocab_dir= VOCAB_DIR)\r\n",
        "\r\n",
        "print('vocab size: ', vocab_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  32000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJj2ELaDON3u"
      },
      "source": [
        "# Load and clear the dialogues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "PpWXsgzBOVMt",
        "outputId": "82524be2-842f-46bf-bb19-b383b8ebd10d"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/dialogues.tsv', sep='\\t')\r\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>persona_1_profile</th>\n",
              "      <th>persona_2_profile</th>\n",
              "      <th>dialogue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;span class=participant_1&gt;–£ –º–µ–Ω—è –ª—é–±–∏–º–∞—è —Ä–∞–±–æ—Ç...</td>\n",
              "      <td>&lt;span class=participant_2&gt;–ò—â—É –ø—Ä–∏–Ω—Ü–∞.&lt;br /&gt;–í–µ–¥...</td>\n",
              "      <td>&lt;span class=participant_2&gt;–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü—Ä–∏–≤...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;span class=participant_1&gt;–Ø —Ä–∞–±–æ—Ç–∞—é —É—á–∏—Ç–µ–ª–µ–º&lt;b...</td>\n",
              "      <td>&lt;span class=participant_2&gt;–Ø –±–∏–∑–Ω–µ—Å–º–µ–Ω&lt;br /&gt;–£ –º...</td>\n",
              "      <td>&lt;span class=participant_1&gt;–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;span class=participant_1&gt;–Ø –∫—É–ø–∏–ª–∞ –¥–æ–º&lt;br /&gt;–Ø ...</td>\n",
              "      <td>&lt;span class=participant_2&gt;–Ø –ø–æ—é –≤ –∫–∞—Ä–∞–æ–∫–µ&lt;br /...</td>\n",
              "      <td>&lt;span class=participant_1&gt;–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;span class=participant_1&gt;—è –≤—Ä–∞—á –∏ –∂–µ–Ω–∞—Ç&lt;br /&gt;...</td>\n",
              "      <td>&lt;span class=participant_2&gt;–Ø –º–∞–ª—å—á–∏–∫&lt;br /&gt;–Ø —É—á—É...</td>\n",
              "      <td>&lt;span class=participant_2&gt;–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ó–¥—Ä–∞...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;span class=participant_1&gt;–Ø —à–∫–æ–ª—å–Ω–∏—Ü–∞.&lt;br /&gt;–Ø ...</td>\n",
              "      <td>&lt;span class=participant_2&gt;–Ø –ø—Ä–æ—Å—Ç–æ–≤–∞—Ç.&lt;br /&gt;–õ—é...</td>\n",
              "      <td>&lt;span class=participant_1&gt;–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   persona_1_profile  ...                                           dialogue\n",
              "0  <span class=participant_1>–£ –º–µ–Ω—è –ª—é–±–∏–º–∞—è —Ä–∞–±–æ—Ç...  ...  <span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü—Ä–∏–≤...\n",
              "1  <span class=participant_1>–Ø —Ä–∞–±–æ—Ç–∞—é —É—á–∏—Ç–µ–ª–µ–º<b...  ...  <span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤...\n",
              "2  <span class=participant_1>–Ø –∫—É–ø–∏–ª–∞ –¥–æ–º<br />–Ø ...  ...  <span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤...\n",
              "3  <span class=participant_1>—è –≤—Ä–∞—á –∏ –∂–µ–Ω–∞—Ç<br />...  ...  <span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ó–¥—Ä–∞...\n",
              "4  <span class=participant_1>–Ø —à–∫–æ–ª—å–Ω–∏—Ü–∞.<br />–Ø ...  ...  <span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSuV5mxHOtti",
        "outputId": "92784dff-5ee5-4312-fc89-bd2cf24814a5"
      },
      "source": [
        "test = data.dialogue.iloc[0]\r\n",
        "print(wrapper.fill(test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü—Ä–∏–≤–µ—Ç) —Ä–∞—Å—Å–∫–∞–∂–∏ –æ\n",
            "—Å–µ–±–µ</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤–µ—Ç) –ø–æ–¥\n",
            "–≤–∫—É—Å–Ω—ã–π –∫–æ—Ñ–µ–µ–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–±–æ–ª—Ç–∞—Ç—å –ø–æ—è–≤–∏–ª–æ—Å—å<br />)</span><br /><span\n",
            "class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ß—Ç–æ —á–∏—Ç–∞–µ—à—å? –ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è\n",
            "–∫–ª–∞—Å—Å–∏–∫–∞</span><br /><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –Ø —Ç–æ–∂–µ\n",
            "–ª—é–±–ª—é –ø–æ–æ–±—â–∞—Ç—å—Å—è</span><br /><span class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1:\n",
            "–õ—é–±–ª—é –∂–∏–≤–æ—Ç–Ω—ã—Ö, –ø—Ä–æ—Å—Ç–æ –æ–±–æ–∂–∞—é, –∫–∞–∫ –∏ —Å–≤–æ—é —Ä–∞–±–æ—Ç—É)</span><br /><span\n",
            "class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –Ø —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫—É –ª—é–±–ª—é</span><br\n",
            "/><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ê —è –≤—ã—Ä–∞—â–∏–≤–∞—é\n",
            "—Ñ–∏–∞–ª–∫–∏</span><br /><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ò –≤–µ–¥—É\n",
            "–∑–¥–æ—Ä–æ–≤—ã–π –∏ –∞–∫—Ç–∏–≤–Ω—ã–π –æ–±—Ä–∞–∑ –∂–∏–∑–Ω–∏!</span><br /><span\n",
            "class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –£—Ö —Ç—ã, –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ.</span><br\n",
            "/><span class=participant_2>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –¢—ã —Å–ª—É—á–∞–π–Ω–æ –Ω–µ –ø—Ä–∏–Ω—Ü –Ω–∞\n",
            "–±–µ–ª–æ–º –∫–æ–Ω–µ? –Ø –µ–≥–æ –æ—á–µ–Ω—å –∂–¥—É<br />..</span><br /><span\n",
            "class=participant_1>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ê —É –º–µ–Ω—è –∏–∑ —Ö–æ–±–±–∏ –∫–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é\n",
            "—Ç—É—Å–∏—Ç—å —Å –º–æ–∏–º –ª—É—á—à–∏–º<br />–¥—Ä—É–≥–æ–º)</span><br />\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK2nRR9WOtwt"
      },
      "source": [
        "def dialogue_clearing(string):\r\n",
        "    '''\r\n",
        "    Clear text from special characters\r\n",
        "    Args:\r\n",
        "        string (string): one dialog text\r\n",
        "    \r\n",
        "    Return\r\n",
        "        list of senteces\r\n",
        "    '''\r\n",
        "    clear_dialogue = []\r\n",
        "    for s in string.split('</span>'):\r\n",
        "        s = re.sub('<span class=participant_1>', '', s)\r\n",
        "        s = re.sub('<span class=participant_2>', '', s)\r\n",
        "        s = re.sub(r'^<br />', '', s)\r\n",
        "        s = re.sub('<br />', ' ', s)\r\n",
        "        s = re.sub(' ,', ',', s)\r\n",
        "        s = re.sub(' \\.', '.', s)\r\n",
        "        s = re.sub(',', ', ', s)\r\n",
        "        s = re.sub('  ', ' ', s)\r\n",
        "        s = re.sub(r'\\r\\n', ' ', s)\r\n",
        "        s = re.sub('\"', '', s)\r\n",
        "        s = re.sub('–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å ', '', s)\r\n",
        "        s = re.sub('–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å ', '', s)\r\n",
        "        \r\n",
        "        if len(s) != 0:\r\n",
        "            clear_dialogue.append(s)\r\n",
        "    \r\n",
        "    return clear_dialogue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lvWTxGeOt01",
        "outputId": "773b92e7-405e-4870-d50b-acaa6a5a09ec"
      },
      "source": [
        "for s in dialogue_clearing(test):\r\n",
        "    print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü—Ä–∏–≤–µ—Ç) —Ä–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤–µ—Ç) –ø–æ–¥ –≤–∫—É—Å–Ω—ã–π –∫–æ—Ñ–µ–µ–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–±–æ–ª—Ç–∞—Ç—å –ø–æ—è–≤–∏–ª–æ—Å—å )\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ß—Ç–æ —á–∏—Ç–∞–µ—à—å? –ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –∫–ª–∞—Å—Å–∏–∫–∞\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –Ø —Ç–æ–∂–µ –ª—é–±–ª—é –ø–æ–æ–±—â–∞—Ç—å—Å—è\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –õ—é–±–ª—é –∂–∏–≤–æ—Ç–Ω—ã—Ö, –ø—Ä–æ—Å—Ç–æ –æ–±–æ–∂–∞—é, –∫–∞–∫ –∏ —Å–≤–æ—é —Ä–∞–±–æ—Ç—É)\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –Ø —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫—É –ª—é–±–ª—é\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ê —è –≤—ã—Ä–∞—â–∏–≤–∞—é —Ñ–∏–∞–ª–∫–∏\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ò –≤–µ–¥—É –∑–¥–æ—Ä–æ–≤—ã–π –∏ –∞–∫—Ç–∏–≤–Ω—ã–π –æ–±—Ä–∞–∑ –∂–∏–∑–Ω–∏!\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –£—Ö —Ç—ã, –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ.\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –¢—ã —Å–ª—É—á–∞–π–Ω–æ –Ω–µ –ø—Ä–∏–Ω—Ü –Ω–∞ –±–µ–ª–æ–º –∫–æ–Ω–µ? –Ø –µ–≥–æ –æ—á–µ–Ω—å –∂–¥—É..\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ê —É –º–µ–Ω—è –∏–∑ —Ö–æ–±–±–∏ –∫–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é —Ç—É—Å–∏—Ç—å —Å –º–æ–∏–º –ª—É—á—à–∏–º –¥—Ä—É–≥–æ–º)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMDGkSU8Ot3-"
      },
      "source": [
        "def dialogue_concat(sentences_list):\r\n",
        "    '''\r\n",
        "    Concatenate several consecutive sentences of one user to one sequence\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        sentences_list (list): list of sentences for one dialogue\r\n",
        "        \r\n",
        "    Return:\r\n",
        "        list of senteces\r\n",
        "    '''\r\n",
        "\r\n",
        "    clear_dialogue_concat = [sentences_list[0]]\r\n",
        "    for i in range(1, len(sentences_list)):\r\n",
        "        s = sentences_list[i]\r\n",
        "        if clear_dialogue_concat[-1][:2] == s[:2]:\r\n",
        "            clear_dialogue_concat[-1] = clear_dialogue_concat[-1] + ' ' + s[3:] \r\n",
        "        else:\r\n",
        "            clear_dialogue_concat.append(s)\r\n",
        "            \r\n",
        "    return clear_dialogue_concat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqR19YiUOt7f",
        "outputId": "01e19e8c-08af-4de3-851f-8a7d1ef46e8c"
      },
      "source": [
        "dialogue = dialogue_clearing(test)\r\n",
        "for s in dialogue_concat(dialogue):\r\n",
        "    print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü—Ä–∏–≤–µ—Ç) —Ä–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤–µ—Ç) –ø–æ–¥ –≤–∫—É—Å–Ω—ã–π –∫–æ—Ñ–µ–µ–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–±–æ–ª—Ç–∞—Ç—å –ø–æ—è–≤–∏–ª–æ—Å—å )\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ß—Ç–æ —á–∏—Ç–∞–µ—à—å? –ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –∫–ª–∞—Å—Å–∏–∫–∞ –Ø —Ç–æ–∂–µ –ª—é–±–ª—é –ø–æ–æ–±—â–∞—Ç—å—Å—è\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –õ—é–±–ª—é –∂–∏–≤–æ—Ç–Ω—ã—Ö, –ø—Ä–æ—Å—Ç–æ –æ–±–æ–∂–∞—é, –∫–∞–∫ –∏ —Å–≤–æ—é —Ä–∞–±–æ—Ç—É) –Ø —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫—É –ª—é–±–ª—é\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ê —è –≤—ã—Ä–∞—â–∏–≤–∞—é —Ñ–∏–∞–ª–∫–∏ –ò –≤–µ–¥—É –∑–¥–æ—Ä–æ–≤—ã–π –∏ –∞–∫—Ç–∏–≤–Ω—ã–π –æ–±—Ä–∞–∑ –∂–∏–∑–Ω–∏!\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –£—Ö —Ç—ã, –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ.\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –¢—ã —Å–ª—É—á–∞–π–Ω–æ –Ω–µ –ø—Ä–∏–Ω—Ü –Ω–∞ –±–µ–ª–æ–º –∫–æ–Ω–µ? –Ø –µ–≥–æ –æ—á–µ–Ω—å –∂–¥—É..\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ê —É –º–µ–Ω—è –∏–∑ —Ö–æ–±–±–∏ –∫–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é —Ç—É—Å–∏—Ç—å —Å –º–æ–∏–º –ª—É—á—à–∏–º –¥—Ä—É–≥–æ–º)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f36K9575PjYe"
      },
      "source": [
        "def swap_users(sentence_list):\r\n",
        "    '''\r\n",
        "    Swap user1 and user2\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        sentences_list (list): list of sentences for one dialogue\r\n",
        "        \r\n",
        "    Return:\r\n",
        "        list of senteces\r\n",
        "    '''\r\n",
        "\r\n",
        "    dialogue = []\r\n",
        "    for s in sentence_list:\r\n",
        "        if s.find('2:') == 0:\r\n",
        "            s = re.sub('2:', '1:', s)\r\n",
        "            dialogue.append(s)\r\n",
        "            continue\r\n",
        "    \r\n",
        "        if s.find('1:') == 0:\r\n",
        "            s = re.sub('1:', '2:', s)\r\n",
        "            dialogue.append(s)\r\n",
        "            continue\r\n",
        "    \r\n",
        "    return dialogue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_I35SjOPu6-",
        "outputId": "6cf30951-3120-4665-f169-4bc991f57f71"
      },
      "source": [
        "dialogue = dialogue_clearing(test)\r\n",
        "dialogue = dialogue_concat(dialogue)\r\n",
        "for s in swap_users(dialogue):\r\n",
        "    print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤–µ—Ç) —Ä–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü—Ä–∏–≤–µ—Ç) –ø–æ–¥ –≤–∫—É—Å–Ω—ã–π –∫–æ—Ñ–µ–µ–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–±–æ–ª—Ç–∞—Ç—å –ø–æ—è–≤–∏–ª–æ—Å—å )\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ß—Ç–æ —á–∏—Ç–∞–µ—à—å? –ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –∫–ª–∞—Å—Å–∏–∫–∞ –Ø —Ç–æ–∂–µ –ª—é–±–ª—é –ø–æ–æ–±—â–∞—Ç—å—Å—è\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –õ—é–±–ª—é –∂–∏–≤–æ—Ç–Ω—ã—Ö, –ø—Ä–æ—Å—Ç–æ –æ–±–æ–∂–∞—é, –∫–∞–∫ –∏ —Å–≤–æ—é —Ä–∞–±–æ—Ç—É) –Ø —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫—É –ª—é–±–ª—é\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ê —è –≤—ã—Ä–∞—â–∏–≤–∞—é —Ñ–∏–∞–ª–∫–∏ –ò –≤–µ–¥—É –∑–¥–æ—Ä–æ–≤—ã–π –∏ –∞–∫—Ç–∏–≤–Ω—ã–π –æ–±—Ä–∞–∑ –∂–∏–∑–Ω–∏!\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –£—Ö —Ç—ã, –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ.\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –¢—ã —Å–ª—É—á–∞–π–Ω–æ –Ω–µ –ø—Ä–∏–Ω—Ü –Ω–∞ –±–µ–ª–æ–º –∫–æ–Ω–µ? –Ø –µ–≥–æ –æ—á–µ–Ω—å –∂–¥—É..\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ê —É –º–µ–Ω—è –∏–∑ —Ö–æ–±–±–∏ –∫–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é —Ç—É—Å–∏—Ç—å —Å –º–æ–∏–º –ª—É—á—à–∏–º –¥—Ä—É–≥–æ–º)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8OGW4v3P0Ca",
        "outputId": "ffffce53-5ec3-4994-de9b-30251631eff3"
      },
      "source": [
        "untokenized_data = []\r\n",
        "dialogue_len = []\r\n",
        "tmp = []\r\n",
        "for i in tqdm(range(data.shape[0])):\r\n",
        "    dialogue = data.dialogue.iloc[i]\r\n",
        "    \r\n",
        "    dialogue = dialogue_clearing(dialogue)\r\n",
        "    dialogue = dialogue_concat(dialogue)\r\n",
        "    \r\n",
        "    if dialogue[0].find('2:') == 0:\r\n",
        "        dialogue = swap_users(dialogue)\r\n",
        "    \r\n",
        "    tmp.append(dialogue)\r\n",
        "    \r\n",
        "    # augmentation\r\n",
        "    if (len(dialogue[2:]) >= 8): tmp.append(dialogue[2:])\r\n",
        "    if (len(dialogue[4:]) >= 8): tmp.append(dialogue[4:])\r\n",
        "    if (len(dialogue[6:]) >= 8): tmp.append(dialogue[6:])\r\n",
        "    if (len(dialogue[8:]) >= 8): tmp.append(dialogue[8:])\r\n",
        "    if (len(dialogue[10:]) >= 8): tmp.append(dialogue[10:])\r\n",
        "    if (len(dialogue[12:]) >= 8): tmp.append(dialogue[12:])\r\n",
        "    if (len(dialogue[14:]) >= 8): tmp.append(dialogue[14:])\r\n",
        "    if (len(dialogue[16:]) >= 8): tmp.append(dialogue[16:])    \r\n",
        "        \r\n",
        "    for d in tmp:    \r\n",
        "        string = ' '.join(d)\r\n",
        "        string = string.strip()\r\n",
        "        string = string + ' STOP\\n'\r\n",
        "        dialogue_len.append(len(d))\r\n",
        "        untokenized_data.append(string)\r\n",
        "        \r\n",
        "    tmp.clear()\r\n",
        "    \r\n",
        "    random.Random(13).shuffle(untokenized_data)\r\n",
        "    \r\n",
        "with open('dialogues2.txt', 'w', encoding='utf-8') as file:\r\n",
        "     file.writelines(untokenized_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10013/10013 [00:04<00:00, 2358.29it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mRW_ERJQFeO",
        "outputId": "3968392b-9c52-4d40-8131-9180a66c38dd"
      },
      "source": [
        "print(wrapper.fill(untokenized_data[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤–µ—Ç) —Ä–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ü—Ä–∏–≤–µ—Ç) –ø–æ–¥\n",
            "–≤–∫—É—Å–Ω—ã–π –∫–æ—Ñ–µ–µ–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–±–æ–ª—Ç–∞—Ç—å –ø–æ—è–≤–∏–ª–æ—Å—å ) –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ß—Ç–æ\n",
            "—á–∏—Ç–∞–µ—à—å? –ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –∫–ª–∞—Å—Å–∏–∫–∞ –Ø —Ç–æ–∂–µ –ª—é–±–ª—é –ø–æ–æ–±—â–∞—Ç—å—Å—è –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2:\n",
            "–õ—é–±–ª—é –∂–∏–≤–æ—Ç–Ω—ã—Ö, –ø—Ä–æ—Å—Ç–æ –æ–±–æ–∂–∞—é, –∫–∞–∫ –∏ —Å–≤–æ—é —Ä–∞–±–æ—Ç—É) –Ø —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫—É –ª—é–±–ª—é\n",
            "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ê —è –≤—ã—Ä–∞—â–∏–≤–∞—é —Ñ–∏–∞–ª–∫–∏ –ò –≤–µ–¥—É –∑–¥–æ—Ä–æ–≤—ã–π –∏ –∞–∫—Ç–∏–≤–Ω—ã–π –æ–±—Ä–∞–∑\n",
            "–∂–∏–∑–Ω–∏! –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –£—Ö —Ç—ã, –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –¢—ã —Å–ª—É—á–∞–π–Ω–æ\n",
            "–Ω–µ –ø—Ä–∏–Ω—Ü –Ω–∞ –±–µ–ª–æ–º –∫–æ–Ω–µ? –Ø –µ–≥–æ –æ—á–µ–Ω—å –∂–¥—É.. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ê —É –º–µ–Ω—è –∏–∑\n",
            "—Ö–æ–±–±–∏ –∫–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é —Ç—É—Å–∏—Ç—å —Å –º–æ–∏–º –ª—É—á—à–∏–º –¥—Ä—É–≥–æ–º)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcD5sF7rQg17"
      },
      "source": [
        "# train tokenizer\r\n",
        "# spm.SentencePieceTrainer.train('--input=dialogues2.txt --pad_id=0 --bos_id=-1 --eos_id=1 --unk_id=2 \\\r\n",
        "#                                 --model_prefix=bpe --vocab_size=32000 --model_type=bpe')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WlMwLaMQSgO"
      },
      "source": [
        "# Data preprocessing and create the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3NK5Hw7QJln",
        "outputId": "f6c147e4-6999-4841-fcbe-1960048c0e52"
      },
      "source": [
        "# loading prepared set to save time\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/dialogues2.txt', 'r', encoding='utf-8') as file:\r\n",
        "     untokenized_data = file.readlines() \r\n",
        "\r\n",
        "untokenized_data = [s.strip() for s in untokenized_data]\r\n",
        "\r\n",
        "print(wrapper.fill(untokenized_data[0]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1: –ê —Ç–µ–±—è –∫–∞–∫ –∑–æ–≤—É—Ç? 2: –í–∏–∫—Ç–æ—Ä 1: –ü—Ä–∏—è—Ç–Ω–æ –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è) 2: –ß–µ–º\n",
            "—É–≤–ª–µ–∫–∞–µ—à—å—Å—è? 1: –ö–µ–º —Ä–∞–±–æ—Ç–∞–µ—à—å? –ß–µ–º —É–≤–ª–µ–∫–∞–µ—à—å—Å—è? 2: –Ø —Ç–∞–∫—Å–∏—Å—Ç –∏ –º–Ω–µ\n",
            "–ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è —Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ –Ω–æ—á–∞–º –ê —á–µ–º —É–≤–ª–µ–∫–∞–µ—à—å—Å—è —Ç—ã? 1: –Ø —Å–ø–æ—Ä—Ç—Å–º–µ–Ω-\n",
            "–ø–ª–∞–≤–µ—Ü, –ª—é–±–ª—é —Ä–∞–±–æ—Ç–∞—Ç—å, –∫–∞–∫ –Ω–∞–¥ —Å–æ–±–æ–π, —Ç–∞–∫ –∏ –≤ —Ü–µ–ª–æ–º –ê —É–≤–ª–µ—á–µ–Ω–∏–µ,\n",
            "–æ—á–µ–Ω—å –≤–∫—É—Å–Ω–æ –≥–æ—Ç–æ–≤–ª—é –£ —Ç–µ–±—è –µ—Å—Ç—å –¥–æ–º–∞—à–Ω–µ–µ –∂–∏–≤–æ—Ç–Ω—ã–µ? 2: –¢—ã –±–æ–ª—å—à–æ–π\n",
            "–º–æ–ª–æ–¥–µ—Ü, –∫—Å—Ç–∞—Ç–∏, —è —É–≤–ª–µ–∫–∞—é—Å—å —Ç–µ–Ω–Ω–∏—Å–æ–º, –∫–∞–∫ —Ç–µ–±–µ —Ç–∞–∫–æ–π –≤–∏–¥ —Å–ø–æ—Ä—Ç–∞? –ù–µ—Ç,\n",
            "–¥–æ–º–∞—à–Ω–∏—Ö –∂–∏–≤–æ—Ç–Ω—ã—Ö –Ω–µ—Ç 1: –ü–ª–∞–≤–∞–Ω—å–µ) –Ø –æ—á–µ–Ω—å —Ö–æ—á—É —Ö–æ–º—è–∫–∞, –∞ –∂–µ–Ω–∞ –Ω–µ\n",
            "—Ä–∞–∑—Ä–µ—à–∞–µ—Ç –ö–∞–∫–æ–µ –∫–∏–Ω–æ –Ω—Ä–∞–≤–∏—Ç—å—Å—è? 2: –≠—ç—ç—Ö, –æ—á–µ–Ω—å –∂–∞–ª—å, –Ω—É –Ω–∏—á–µ–≥–æ, –º–æ–∂–µ—Ç\n",
            "–∫–æ–≥–¥–∞- –Ω–∏–±—É–¥—å —Ç—ã –∏—Å–ø–æ–ª–Ω–∏—Ç—å —Å–≤–æ—é –º–µ—á—Ç—É 1: –•–æ—Ç–µ–ª–æ—Å—å –±—ã) 2: –ö–∏–Ω–æ –Ω–µ\n",
            "–ª—é–±–ª—é, —è –±–æ–ª—å—à–µ –º—É–∑—ã–∫—É —Å–ª—É—à–∞—é, –≤ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —à–∞–Ω—Å–æ–Ω 1: –£ —Ç–µ–±—è –µ—Å—Ç—å\n",
            "–º–µ—á—Ç–∞? –ê —è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —Ö–æ–∂—É –≤ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä—ã –Ω–∞ –ø—Ä–∏–º–µ—Ç–Ω—ã–µ) 2: –î–∞, –∫–æ–Ω–µ—á–Ω–æ, –∞\n",
            "—É —Ç–µ–±—è? –ï—Å—Ç—å –∫–∞–∫–æ–π-—Ç–æ –ª—é–±–∏–º—ã–π –∂–∞–Ω—Ä? –ö—Å—Ç–∞—Ç–∏, —è –µ—â–µ –ª—é–±–ª—é —á–∏—Ç–∞—Ç—å –ê —Ç—ã\n",
            "–ª—é–±–∏—à—å –∫–Ω–∏–≥–∏? 1: –î–∞, –º–∏—Å—Ç–∏–∫–∞ –ö–æ–≥–¥–∞ –≤—Å–µ –Ω–∞ –≥—Ä–∞–Ω–µ) –ê –∫–∞–∫–∏–µ –∫–Ω–∏–≥–∏ –ª—é–±–∏—à—å?\n",
            "2: –û–æ–æ, –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –∑–≤—É—á–∏—Ç 1: –¢–æ–∂–µ, –≤—Å–µ–≥–æ –ë—É–ª–≥–∞–∫–æ–≤–∞ –ø–µ—Ä–µ—á–∏—Ç–∞–ª 2:\n",
            "–î–µ—Ç–µ–∫—Ç–∏–≤—ã –û–æ–æ –ó–¥–æ—Ä–æ–≤–æ 1: –£–±–∏–π—Å—Ç–≤–æ –≤ –ø–æ–ª—è—Ä–Ω–æ–º —ç–∫—Å–ø—Ä–µ—Å—Å —á–∏—Ç–∞–ª? 2: –ù–µ—Ç –ê\n",
            "—Ç—ã? 1: –û—á–µ–Ω—å –∫—Ä—É—Ç–∞—è –∫–Ω–∏–≥–∞ –†–µ–∫–æ–º–µ–Ω–¥—É—é) 2: –í–æ–∑—å–º—É –Ω–∞ –∑–∞–º–µ—Ç–∫—É 1: –ü—Ä–∏—è—Ç–Ω–æ\n",
            "–±—ã–ª–æ –ø–æ–æ–±—â–∞—Ç—å—Å—è 2: –ò –º–Ω–µ –ü–æ–∫–∞ STOP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHPIJPH6E3dT"
      },
      "source": [
        "# Attention! do not run\r\n",
        "# random.shuffle(untokenized_data)\r\n",
        "# print(wrapper.fill(untokenized_data[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqxEykiLRhoo",
        "outputId": "10571b39-a518-456e-f290-b5494644aaf4"
      },
      "source": [
        "# train/eval split\r\n",
        "margin = int(len(untokenized_data)*0.95)\r\n",
        "\r\n",
        "train_dialogues = untokenized_data[:margin]\r\n",
        "print('train cases: ', len(train_dialogues))\r\n",
        "eval_dialogues = untokenized_data[margin:]\r\n",
        "print('eval cases: ', len(eval_dialogues))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train cases:  44460\n",
            "eval cases:  2341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rlD2rxFQx6A"
      },
      "source": [
        "# clear mem\r\n",
        "# del untokenized_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W724qoS9R783"
      },
      "source": [
        "def data_generator(data, shuffle=True):\r\n",
        "    '''\r\n",
        "      Input: \r\n",
        "        data - list of dialogues\r\n",
        "        shuffle - If True: shuffle the data order\r\n",
        "      Output:\r\n",
        "        a tuple containing 2 elements:\r\n",
        "        article\r\n",
        "        summary\r\n",
        "    '''\r\n",
        "    \r\n",
        "    data_lng = len(data) # len(data)\r\n",
        "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\r\n",
        "\r\n",
        "    if shuffle:\r\n",
        "        random.shuffle(index_list) # re-shuffle the order\r\n",
        "    \r\n",
        "    index = 0 # Start with the first element\r\n",
        "    while True:\r\n",
        "        # Wrap the index each time that we reach the end of the list\r\n",
        "        if index >= data_lng:\r\n",
        "            index = 0\r\n",
        "            if shuffle:\r\n",
        "                random.shuffle(index_list) # re-shuffle the order\r\n",
        "            \r\n",
        "        sample = data[index_list[index]]\r\n",
        "        index += 1\r\n",
        "        yield(sample, sample) # input and target\r\n",
        "\r\n",
        "# create data streams\r\n",
        "def train_data_stream():\r\n",
        "    return data_generator(train_dialogues, shuffle=True)\r\n",
        "\r\n",
        "def eval_data_stream():\r\n",
        "    return data_generator(eval_dialogues, shuffle=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE0jmPywSPbI"
      },
      "source": [
        "def detokenize(integers):\r\n",
        "    s = trax.data.detokenize(\r\n",
        "        integers,\r\n",
        "        vocab_type=VOCAB_TYPE,\r\n",
        "        vocab_file=VOCAB_FILE,\r\n",
        "        vocab_dir=VOCAB_DIR) # loading pre-prepared model to save time\r\n",
        "    return s\r\n",
        "\r\n",
        "\r\n",
        "def tokenize(s):\r\n",
        "    inputs =  next(trax.data.tokenize(\r\n",
        "        iter([s]),\r\n",
        "        vocab_type=VOCAB_TYPE,\r\n",
        "        vocab_file=VOCAB_FILE,\r\n",
        "        vocab_dir=VOCAB_DIR))\r\n",
        "    \r\n",
        "    return list(inputs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHOE73afSunn"
      },
      "source": [
        "# trax allows us to use combinators to generate our data pipeline\r\n",
        "data_pipeline = trax.data.Serial(\r\n",
        "    \r\n",
        "    # tokenize the data\r\n",
        "    trax.data.Tokenize(vocab_type=VOCAB_TYPE,\r\n",
        "                       vocab_dir=VOCAB_DIR,\r\n",
        "                       vocab_file=VOCAB_FILE),\r\n",
        "    \r\n",
        "    # filter too long sequences\r\n",
        "    trax.data.FilterByLength(max_length=2048, min_length=64),\r\n",
        "    \r\n",
        "    # bucket by length\r\n",
        "    trax.data.BucketByLength(boundaries=[128, 256, 512, 1024],\r\n",
        "                             batch_sizes=[16, 8, 4, 2, 1]),\r\n",
        "    \r\n",
        "    # add loss weights but do not add it to the padding tokens (i.e. 0)\r\n",
        "    trax.data.AddLossWeights(id_to_mask=0)\r\n",
        ")\r\n",
        "\r\n",
        "# apply the data pipeline to our train and eval sets\r\n",
        "train_stream = data_pipeline(data_generator(train_dialogues))\r\n",
        "eval_stream = data_pipeline(data_generator(eval_dialogues))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlZrmKpkUrYR",
        "outputId": "cdf9fe97-8b68-4dae-a25a-46c68f979eca"
      },
      "source": [
        "# the stream generators will yield (input, target, weights). let's just grab the input for inspection\r\n",
        "inp, _, m = next(train_stream)\r\n",
        "\r\n",
        "# print the shape. format is (batch size, token length)\r\n",
        "print(\"input shape: \", inp.shape)\r\n",
        "\r\n",
        "# detokenize the first element\r\n",
        "print(detokenize(inp[0]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape:  (8, 256)\n",
            "1: –ù–∞–¥–æ–µ–ª–∞ –∑–∏–º–∞, —Å–∫–æ—Ä–µ–µ –±—ã –ª–µ—Ç–æ. –õ—é–±–ª—é –µ–≥–æ –æ—á–µ–Ω—å 2: –í—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ? 1: –°–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —á—Ç–æ —è —Ä–∞–±–æ—Ç–∞—é –ö–∞–∫ –≤ –∂–µ –≤ –≥–æ—Ä–æ–¥–µ –≤ –∫–æ—Ç–æ—Ä–æ–º —è –∂–∏–≤—É –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ ? üòä –ê—Å—è, –≤—ã –±—ã–ª–∏ –≤ –≥–æ—Ä–∞—Ö? –Ø –≤–æ—Ç –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –±—ã–ª–∞ –Ω–æ –æ—á–µ–Ω—å —Ö–æ—á—É –≤ –¢–∏–±–µ—Ç, –ª–∏–±–æ –Ω–∞ –ì–∏–º–∞–ª–∞–∏, –Ω–æ –Ω–∞ —Ö—É–¥–æ–π –∫–æ–Ω–µ—Ü –∏ —É—Ä–∞–ª—å—Å–∫–∏–π —Ö—Ä–µ–±–µ—Ç —Å–æ–π–¥–µ—Ç üòä –ê —á–µ–º –≤—ã –∑–∞–Ω–∏–º–∞–µ—Ç–µ—Å—å? 2: –Ø –º–µ–¥—Å–µ—Å—Ç—Ä–∞ –∏ —Å—Ç–µ–Ω–¥–∞–ø-–∫–æ–º–∏–∫, –ª—é–±–ª—é –∂–∏–≤–æ—Ç–Ω—ã—Ö –ê –≤—ã? –£ –º–µ–Ω—è 4 —Å–æ–±–∞–∫–∏ 1: –Ø –æ—á–µ–Ω—å –ª—é–±–ª—é –∫–æ—à–µ–∫, –∏ —É –º–µ–Ω—è –æ–¥–Ω–∞ –µ—Å—Ç—å –ù–µ –º–Ω–æ–≥–æ –ª–∏ —Å–æ–±–∞–∫ —É –≤–∞—Å? 2: –î—É–º–∞—é, —á—Ç–æ –Ω–µ—Ç. –í—ã –∑–∞–º—É–∂–µ–º? 1: –ù–µ—Ç –Ω–µ –∑–∞–º—É–∂–µ–º, —É –º–µ–Ω—è –∂–µ –∫–æ—Ç –µ—Å—Ç—åüòÄ –ê –µ—â—ë —è –ª—é–±–ª—é —Ñ—Ä—É–∫—Ç—ã 2: –ê —è –≤–æ—Ç –≤—ã—à–ª–∞ –∑–∞–º—É–∂ –∑–∞ –¥—Ä—É–≥–∞)–ê –≤—ã —é–º–æ—Ä–∏—Å—Ç–∫–∞. 1: –û—Å–æ–±–µ–Ω–Ω–æ –º–∞–Ω–≥–æ –∏ –∞–ø–µ–ª—å—Å–∏–Ω—ã 2: –õ—é–±–∏—Ç–µ –≥–æ—Ç–æ–≤–∏—Ç—å? 1: –¢–µ—Ä–ø–µ—Ç—å –Ω–µ –º–æ–≥—É –≥–æ—Ç–æ–≤–∏—Ç—å –ê —é–º–æ—Ä–∏—Ç—å –º–æ—ë —Ö–æ–±–±–∏üòä STOP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQr1gruD3XQ2",
        "outputId": "2bb3d882-a1d9-4571-cc9e-59068df418af"
      },
      "source": [
        "m[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZz6hVaDfvOJ"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSkIwZxDfdux"
      },
      "source": [
        "def ReformerLM(vocab_size=32000, n_layers=2, mode='train', attention_type=tl.SelfAttention):\r\n",
        "    \"\"\"\r\n",
        "    Args: \r\n",
        "        vocab_size (int): size of the vocabulary\r\n",
        "        n_layers (int): number of decoder layers\r\n",
        "        mode (string): setting of the model which can be 'train', 'eval', or 'predict' \r\n",
        "        attention_type(class): attention class to use \r\n",
        "    Returns: \r\n",
        "        model (TransformerLM): a transformer language model implemented in Trax\r\n",
        "    \"\"\"    \r\n",
        "    # initialize an instance of Trax's TransformerLM class\r\n",
        "    model = trax.models.ReformerLM( \r\n",
        "        vocab_size=vocab_size,\r\n",
        "        # d_model=d_model,\r\n",
        "        # d_ff=d_ff,\r\n",
        "        n_layers=n_layers,\r\n",
        "        # max_len=max_len,\r\n",
        "        # dropout = dropout,\r\n",
        "        mode = mode,\r\n",
        "        attention_type=attention_type\r\n",
        "    )\r\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bykCKvza2Yw",
        "outputId": "48c65db6-6e73-43fe-e77d-67b231158191",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tmp_model = ReformerLM(n_layers=2, mode='train', attention_type=tl.SelfAttention)\r\n",
        "tmp_model.sublayers[-1] = tl.Serial(tl.Dense(vocab_size), tl.LogSoftmax())\r\n",
        "print(tmp_model)\r\n",
        "del tmp_model"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_32000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Dup_out2\n",
            "  ReversibleSerial_in2_out2[\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "  ]\n",
            "  Concatenate_in2\n",
            "  LayerNorm\n",
            "  Dropout\n",
            "  Serial[\n",
            "    Dense_32000\n",
            "    LogSoftmax\n",
            "  ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th5ZANZZRi4X"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxIxaaotbqLN"
      },
      "source": [
        "ChatModel = ReformerLM(vocab_size=32000, n_layers=6, mode='train')\r\n",
        "ChatModel.sublayers[-1] = tl.Serial(tl.Dense(vocab_size), tl.LogSoftmax())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGDjwzKgRhkn"
      },
      "source": [
        "def training_loop(ChatModel, train_gen, eval_gen, output_dir = \"~/model/\"):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "        ChatModel:  the Reformer language model with LogSoftmax output\r\n",
        "        train_gen (generator): train data generator.\r\n",
        "        eval_gen (generator): Validation generator. \r\n",
        "        output_dir (string): Path to save the model output. Defaults to './model/'.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        trax.supervised.training.Loop: Training loop for the model.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    output_dir = os.path.expanduser(output_dir)\r\n",
        "\r\n",
        "    # use the warmup_and_rsqrt_decay learning rate schedule\r\n",
        "    # lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=4000, max_value=0.0002)\r\n",
        "\r\n",
        "    train_task = training.TrainTask(            \r\n",
        "        labeled_data=train_gen,\r\n",
        "        loss_layer=tl.CrossEntropyLoss(),\r\n",
        "        optimizer=trax.optimizers.Adam(2e-4),\r\n",
        "        # lr_schedule=lr_schedule,\r\n",
        "        n_steps_per_checkpoint=10\r\n",
        "    )\r\n",
        "\r\n",
        "    eval_task = training.EvalTask(                      \r\n",
        "        labeled_data=eval_gen,\r\n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]\r\n",
        "    )\r\n",
        "\r\n",
        "    loop = training.Loop(ChatModel,\r\n",
        "                         train_task,\r\n",
        "                         eval_tasks=[eval_task],\r\n",
        "                         output_dir=output_dir)\r\n",
        "    return loop"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fml0UGRASDoM"
      },
      "source": [
        "!cp /content/drive/MyDrive/model2/model_6_8_02_100.pkl.gz ~/model/model.pkl.gz"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t67oYGO5sxK1"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU1tQpq0sya6"
      },
      "source": [
        "%tensorboard --logdir ../root/model/ # train = name of folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gX2SUFARd3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbd6a42-6f0f-4ef9-85ac-e9a3147ce629"
      },
      "source": [
        "# !rm -f model/model.pkl.gz\r\n",
        "loop = training_loop(ChatModel, train_stream, eval_stream)\r\n",
        "loop.run(100)\r\n",
        "!cp ~/model/model.pkl.gz /content/drive/MyDrive/model2/rmodel_test.pkl.gz"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 69136640\n",
            "Step      1: Ran 1 train steps in 124.59 secs\n",
            "Step      1: train CrossEntropyLoss |  10.40490532\n",
            "Step      1: eval  CrossEntropyLoss |  10.19735813\n",
            "Step      1: eval          Accuracy |  0.00213068\n",
            "\n",
            "Step     10: Ran 9 train steps in 370.62 secs\n",
            "Step     10: train CrossEntropyLoss |  9.49746132\n",
            "Step     10: eval  CrossEntropyLoss |  8.87853241\n",
            "Step     10: eval          Accuracy |  0.05470914\n",
            "\n",
            "Step     20: Ran 10 train steps in 158.22 secs\n",
            "Step     20: train CrossEntropyLoss |  8.10419369\n",
            "Step     20: eval  CrossEntropyLoss |  7.84483194\n",
            "Step     20: eval          Accuracy |  0.04067197\n",
            "\n",
            "Step     30: Ran 10 train steps in 160.80 secs\n",
            "Step     30: train CrossEntropyLoss |  7.36917973\n",
            "Step     30: eval  CrossEntropyLoss |  7.04085541\n",
            "Step     30: eval          Accuracy |  0.04953999\n",
            "\n",
            "Step     40: Ran 10 train steps in 159.63 secs\n",
            "Step     40: train CrossEntropyLoss |  7.06333065\n",
            "Step     40: eval  CrossEntropyLoss |  7.10729074\n",
            "Step     40: eval          Accuracy |  0.05211480\n",
            "\n",
            "Step     50: Ran 10 train steps in 157.52 secs\n",
            "Step     50: train CrossEntropyLoss |  6.98024750\n",
            "Step     50: eval  CrossEntropyLoss |  6.90197611\n",
            "Step     50: eval          Accuracy |  0.04192355\n",
            "\n",
            "Step     60: Ran 10 train steps in 159.19 secs\n",
            "Step     60: train CrossEntropyLoss |  7.04194784\n",
            "Step     60: eval  CrossEntropyLoss |  7.21898556\n",
            "Step     60: eval          Accuracy |  0.05259313\n",
            "\n",
            "Step     70: Ran 10 train steps in 159.38 secs\n",
            "Step     70: train CrossEntropyLoss |  7.04204178\n",
            "Step     70: eval  CrossEntropyLoss |  6.85829401\n",
            "Step     70: eval          Accuracy |  0.05903790\n",
            "\n",
            "Step     80: Ran 10 train steps in 158.89 secs\n",
            "Step     80: train CrossEntropyLoss |  7.02523947\n",
            "Step     80: eval  CrossEntropyLoss |  7.50239658\n",
            "Step     80: eval          Accuracy |  0.04988488\n",
            "\n",
            "Step     90: Ran 10 train steps in 159.04 secs\n",
            "Step     90: train CrossEntropyLoss |  7.06895685\n",
            "Step     90: eval  CrossEntropyLoss |  6.91415501\n",
            "Step     90: eval          Accuracy |  0.04988831\n",
            "\n",
            "Step    100: Ran 10 train steps in 160.14 secs\n",
            "Step    100: train CrossEntropyLoss |  7.07383633\n",
            "Step    100: eval  CrossEntropyLoss |  7.07785559\n",
            "Step    100: eval          Accuracy |  0.04872007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNILzgz6Ls8F"
      },
      "source": [
        "# !cp ~/model/model.pkl.gz /content/drive/MyDrive/model2/\r\n",
        "# !cp ~/model/model.pkl.gz /content/drive/MyDrive/model2/model_6_8_02_20.pkl.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8rd7b7oKK12"
      },
      "source": [
        "## GPU memory footprint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx3fiEFMtm7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31f1db09-a6f6-4a2c-d79a-9131da94612b"
      },
      "source": [
        "# memory footprint support libraries/code\r\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\r\n",
        "!pip install gputil\r\n",
        "!pip install psutil\r\n",
        "!pip install humanize\r\n",
        "import psutil\r\n",
        "import humanize\r\n",
        "import os\r\n",
        "import GPUtil as GPU\r\n",
        "GPUs = GPU.getGPUs()\r\n",
        "# XXX: only one GPU on Colab and isn‚Äôt guaranteed\r\n",
        "gpu = GPUs[0]\r\n",
        "def printm():\r\n",
        " process = psutil.Process(os.getpid())\r\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\r\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\r\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=fdef2c61bbcead285449bcced893547d3c20f505352fb175df3fa432fc633ea8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Gen RAM Free: 10.7 GB  | Proc size: 4.2 GB\n",
            "GPU RAM Free: 568MB | Used: 7043MB | Util  93% | Total 7611MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxsr9DDYiKk"
      },
      "source": [
        "# Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAgq5k-d267N"
      },
      "source": [
        "## Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg6247VCucGY"
      },
      "source": [
        "!cp /content/drive/MyDrive/model2/model.pkl.gz ~/model/model.pkl.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKiyx7aSwjha"
      },
      "source": [
        "# define the `predict_mem_len` and `predict_drop_len` of tl.SelfAttention\r\n",
        "def attention(*args, **kwargs):\r\n",
        "    # number of input positions to remember in a cache when doing fast inference. \r\n",
        "    kwargs['predict_mem_len'] = 120\r\n",
        "    # number of input elements to drop once the fast inference input cache fills up.\r\n",
        "    kwargs['predict_drop_len'] = 120\r\n",
        "    # return the attention layer with the parameters defined above\r\n",
        "    return tl.SelfAttention(*args, **kwargs)\r\n",
        "\r\n",
        "# define the model using the ReformerLM function you implemented earlier.\r\n",
        "ChatModel = ReformerLM(\r\n",
        "    vocab_size=32000,\r\n",
        "    n_layers=6,\r\n",
        "    mode='predict',\r\n",
        "    attention_type=attention,\r\n",
        ")\r\n",
        "\r\n",
        "# define an input signature so we can initialize our model. shape will be (1, 1) and the data type is int32.\r\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZDLV6et4Ds"
      },
      "source": [
        "# define the model using the ReformerLM function you implemented earlier.\r\n",
        "# ChatModel = TransformerLM(n_layers=6, mode='predict')\r\n",
        "# ChatModel.sublayers[-1] = tl.Serial(tl.Dense(vocab_size), tl.LogSoftmax())\r\n",
        "\r\n",
        "# define an input signature so we can initialize our model. shape will be (1, 1) and the data type is int32.\r\n",
        "# shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tuxn1_Dt4Nr",
        "outputId": "484efd4d-7c2b-4d18-b3a3-93f31bbf442e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# initialize from file\r\n",
        "ChatModel.init_from_file('../root/model/model.pkl.gz', weights_only=True, input_signature=shape11)\r\n",
        "# ChatModel.init_from_file('/content/drive/MyDrive/model2/model_6_8_02_100.pkl.gz', weights_only=True, input_signature=shape11)\r\n",
        "\r\n",
        "# save the starting state\r\n",
        "STARTING_STATE = ChatModel.state"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LayerError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLayerError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-37111effe97a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initialize from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mChatModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../root/model/model.pkl.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# ChatModel.init_from_file('/content/drive/MyDrive/model2/model_6_8_02_100.pkl.gz', weights_only=True, input_signature=shape11)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# save the starting state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36minit_from_file\u001b[0;34m(self, file_name, weights_only, input_signature)\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, input_signature, rng, use_cache)\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_short_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m       raise LayerError(name, 'init', self._caller,\n\u001b[0;32m--> 307\u001b[0;31m                        input_signature, trace) from None\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minit_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLayerError\u001b[0m: Exception passing through layer Serial (in init):\n  layer created in file [...]/models/reformer/reformer.py, line 185\n  layer input shapes: ShapeDtype{shape:(1, 1), dtype:<class 'numpy.int32'>}\n\n  File [...]/trax/layers/combinators.py, line 105, in init_weights_and_state\n    sublayer.init(inputs, use_cache=True))\n\nLayerError: Exception passing through layer ReversibleSerial (in init):\n  layer created in file [...]/models/reformer/reformer.py, line 179\n  layer input shapes: (ShapeDtype{shape:(1, 1, 512), dtype:float32}, ShapeDtype{shape:(1, 1, 512), dtype:float32})\n\n  File [...]/trax/layers/combinators.py, line 105, in init_weights_and_state\n    sublayer.init(inputs, use_cache=True))\n\nLayerError: Exception passing through layer ReversibleHalfResidual (in init):\n  layer created in file [...]/models/reformer/reformer.py, line 69\n  layer input shapes: (ShapeDtype{shape:(1, 1, 512), dtype:float32}, ShapeDtype{shape:(1, 1, 512), dtype:float32})\n\n  File [...]/trax/layers/reversible.py, line 326, in init_weights_and_state\n    attn_weights, attn_state = self.attention_layer.init(inputs)\n\nLayerError: Exception passing through layer SelfAttention (in init):\n  layer created in file [...]/layers/research/efficient_attention.py, line 1009\n  layer input shapes: ShapeDtype{shape:(1, 1, 512), dtype:float32}\n\n  File [...]/layers/research/efficient_attention.py, line 1072, in init_weights_and_state\n    self.rng, weights_rng, state_rng = fastmath.random.split(self.rng, 3)\n\n  File [...]/trax/fastmath/ops.py, line 71, in split\n    return backend()['random_split'](prng, num)\n\n  File [...]/jax/_src/random.py, line 260, in split\n    return _split(key, int(num))  # type: ignore\n\n  File [...]/jax/_src/traceback_util.py, line 139, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n\n  File [...]/dist-packages/jax/api.py, line 416, in f_jitted\n    return cpp_jitted_f(context, *args, **kwargs)\n\n  File [...]/dist-packages/jax/api.py, line 303, in cache_miss\n    donated_invars=donated_invars)\n\n  File [...]/dist-packages/jax/core.py, line 1394, in bind\n    return call_bind(self, fun, *args, **params)\n\n  File [...]/dist-packages/jax/core.py, line 1379, in call_bind\n    top_trace = find_top_trace(args)\n\n  File [...]/dist-packages/jax/core.py, line 818, in find_top_trace\n    top_tracer._assert_live()\n\n  File [...]/jax/interpreters/partial_eval.py, line 940, in _assert_live\n    raise core.escaped_tracer_error(self, None)\n\njax.core.UnexpectedTracerError: Encountered an unexpected tracer. Perhaps this tracer escaped through global state from a previously traced function.\nThe functions being transformed should not save traced values to global state.\n\nThe tracer that caused this error was created on line /usr/local/lib/python3.7/dist-packages/trax/layers/research/efficient_attention.py:1072 (init_weights_and_state).\nWhen the tracer was created, the final 5 stack frames (most recent last) excluding JAX-internal frames were:\n\n/usr/local/lib/python3.7/dist-packages/trax/layers/combinators.py:105 (init_weights_and_state)\n/usr/local/lib/python3.7/dist-packages/trax/layers/base.py:294 (init)\n\n/usr/local/lib/python3.7/dist-packages/trax/layers/reversible.py:326 (init_weights_and_state)\n/usr/local/lib/python3.7/dist-packages/trax/layers/base.py:294 (init)\n\n/usr/local/lib/python3.7/dist-packages/trax/layers/research/efficient_attention.py:1072 (init_weights_and_state)\nThe function being traced when the tracer leaked was call_wrapped at /usr/local/lib/python3.7/dist-packages/jax/linear_util.py:152.\n\nTo catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX5oCsq-dC_7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdnmqmftdLk-"
      },
      "source": [
        "## Full Text Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ7aZKaRt4U8"
      },
      "source": [
        "def output_gen(ChatModel, start_sentence, temperature, accelerate=True):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "        ChatModel:  the Transformer language model\r\n",
        "        start_sentence (string): starting sentence of the conversation\r\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\r\n",
        "            0.0: same as argmax, always pick the most probable token\r\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        generator: yields the next symbol generated by the model\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    input_tokens = tokenize(start_sentence)\r\n",
        "    \r\n",
        "    # Add batch dimension to array\r\n",
        "    input_tokens_with_batch = np.expand_dims(input_tokens, axis=0)\r\n",
        "    \r\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \r\n",
        "        ChatModel,\r\n",
        "        inputs=input_tokens_with_batch,\r\n",
        "        temperature=temperature,\r\n",
        "        accelerate=accelerate\r\n",
        "    )\r\n",
        "\r\n",
        "    return output_gen"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQvs9Hh2voXg"
      },
      "source": [
        "def generate_dialogue(ChatModel, model_state, start_sentence, max_len, temperature, accelerate):\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "        ChatModel:  the Transformer language model\r\n",
        "        model_state (np.array): initial state of the model before decoding\r\n",
        "        start_sentence (string): starting sentence of the conversation\r\n",
        "        max_len (int): maximum number of tokens to generate \r\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\r\n",
        "            0.0: same as argmax, always pick the most probable token\r\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        generator: yields the next symbol generated by the model\r\n",
        "    \"\"\"  \r\n",
        "    \r\n",
        "    delimiter_1 = '1:' \r\n",
        "    delimiter_2 = '2:'\r\n",
        "    \r\n",
        "    # initialize detokenized output\r\n",
        "    sentence = ''\r\n",
        "    \r\n",
        "    # token counter\r\n",
        "    counter = 0\r\n",
        "\r\n",
        "    result = []\r\n",
        "    \r\n",
        "    # reset the model state when starting a new dialogue\r\n",
        "    ChatModel.state = model_state\r\n",
        "    \r\n",
        "    # calls the output generator\r\n",
        "    output = output_gen(ChatModel, start_sentence, temperature=temperature, accelerate=accelerate)\r\n",
        "    \r\n",
        "    # print the starting sentence\r\n",
        "    print(start_sentence.split(delimiter_2)[0])\r\n",
        "\r\n",
        "    control = []\r\n",
        "    \r\n",
        "    \r\n",
        "    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.\r\n",
        "    for o in output:\r\n",
        "        \r\n",
        "        result.append(o)\r\n",
        "        control.append(int(o))\r\n",
        "        \r\n",
        "        sentence = detokenize(np.concatenate(result, axis=0))\r\n",
        "\r\n",
        "        if sentence.endswith(delimiter_1):\r\n",
        "            sentence = sentence.split(delimiter_1)[0]\r\n",
        "            print(delimiter_2 + ' ' + sentence)\r\n",
        "            sentence = ''\r\n",
        "            result.clear()\r\n",
        "        \r\n",
        "        elif sentence.endswith(delimiter_2):\r\n",
        "            sentence = sentence.split(delimiter_2)[0]\r\n",
        "            print(delimiter_1 + ' ' + sentence)\r\n",
        "            sentence = ''\r\n",
        "            result.clear()\r\n",
        "\r\n",
        "        counter += 1\r\n",
        "        \r\n",
        "        if counter > max_len:\r\n",
        "            break\r\n",
        "\r\n",
        "        if o[0] == 139:\r\n",
        "            break\r\n",
        "\r\n",
        "\r\n",
        "    return detokenize(control), control"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjJKNcfesBS5"
      },
      "source": [
        "sample_sentence = '1: –ü—Ä–∏–≤–µ—Ç) 2: –ü—Ä–∏–≤–µ—Ç!'\r\n",
        "control1, control2 = generate_dialogue(ChatModel, model_state=STARTING_STATE, start_sentence=sample_sentence, max_len=200, temperature=0.0, accelerate=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oO6jRlTBWX2"
      },
      "source": [
        "sample_sentence = '1: –ö–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç? 2:'\r\n",
        "_, _ = generate_dialogue(ChatModel, model_state=STARTING_STATE, start_sentence=sample_sentence, max_len=200, temperature=0.2, accelerate=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Op3G2dBWep"
      },
      "source": [
        "sample_sentence = '1: –î–æ–±—Ä—ã–π –≤–µ—á–µ—Ä 2: –î–æ–±—Ä–æ–µ —É—Ç—Ä–æ! 1: –ö–æ–≥–¥–∞ —É–≤–∏–¥–∏–º—Å—è?'\r\n",
        "_, _ = generate_dialogue(ChatModel, model_state=STARTING_STATE, start_sentence=sample_sentence, max_len=200, temperature=0.0, accelerate=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM30ml04bg4x"
      },
      "source": [
        "## Interactive Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X6GvIPRblKF"
      },
      "source": [
        "def next_symbol(model, input_tokens, temperature=0.0):\r\n",
        "    \"\"\"Returns the next symbol for a given sentence.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        input_tokens (list): tokenized sentence\r\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        int: tokenized symbol.\r\n",
        "    \"\"\"\r\n",
        "    batch = np.expand_dims(input_tokens, axis=0)\r\n",
        "\r\n",
        "    logits = model(batch)\r\n",
        "    # print(logits.shape)\r\n",
        "    logits = logits[:, -1, :][0]\r\n",
        "    sample = tl.logsoftmax_sample(logits, temperature=temperature)\r\n",
        "    # sample = int(np.argmax(logits))\r\n",
        "    # print(logits[0])\r\n",
        "    \r\n",
        "    return sample"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSyuDiAcyMGL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fc0804ed-8875-4a03-c576-5bf84784ac49"
      },
      "source": [
        "ChatModel.state = STARTING_STATE\r\n",
        "cur_output_tokens = tokenize('1: –î–æ–±—Ä—ã–π –≤–µ—á–µ—Ä 2: –ö–∞–∫')\r\n",
        "detokenize([next_symbol(ChatModel, cur_output_tokens)])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'–¥–µ–ª–∞'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3aYkPD2bq6c"
      },
      "source": [
        "def interactive_decode_bot_first(model, model_state, greeting='–ü—Ä–∏–≤–µ—Ç', accelerate=False):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "        model_state: initial state of the model before decoding\r\n",
        "        greeting (string):\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        string: summary of the input.\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    delimiter_1 = '1:' \r\n",
        "    delimiter_2 = '2:'\r\n",
        "    input_sentence = ''\r\n",
        "\r\n",
        "    cur_output_tokens = tokenize('1: ' + greeting + ' 2:')\r\n",
        "    result = cur_output_tokens.copy()[1:]\r\n",
        "    cur_output = 0\r\n",
        "    stop = 129\r\n",
        "\r\n",
        "    fast_model = tl.Accelerate(model) if accelerate else model\r\n",
        "\r\n",
        "    while (cur_output != stop or input_sentence != 'STOP'):\r\n",
        "\r\n",
        "        # clear past diolog state\r\n",
        "        fast_model.state = model_state\r\n",
        "\r\n",
        "        result.append(cur_output)\r\n",
        "        sentence = detokenize(result)\r\n",
        "\r\n",
        "        # if sentence.endswith(delimiter_1):\r\n",
        "        #     sentence = sentence.split(delimiter_1)[0]\r\n",
        "        #     print(delimiter_2 + ' ' + sentence)\r\n",
        "        #     sentence = ''\r\n",
        "        #     result.clear()\r\n",
        "        \r\n",
        "        if sentence.endswith(delimiter_2):\r\n",
        "            sentence = sentence.split(delimiter_2)[0]\r\n",
        "            print(delimiter_1 + ' ' + sentence)\r\n",
        "            sentence = ''\r\n",
        "            result.clear()\r\n",
        "\r\n",
        "            input_sentence = input('2: ')\r\n",
        "            input_sentence = tokenize(input_sentence)\r\n",
        "            cur_output_tokens += input_sentence + [9]\r\n",
        "            \r\n",
        "\r\n",
        "        cur_output = next_symbol(fast_model, cur_output_tokens)\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "\r\n",
        "    return None"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj0AlOpFbr2Y"
      },
      "source": [
        "interactive_decode_bot_first(ChatModel, STARTING_STATE, greeting='–î–æ–±—Ä—ã–π –≤–µ—á–µ—Ä')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5k2h6Habr7Z"
      },
      "source": [
        "def interactive_decode_bot_second(model, model_state):\r\n",
        "    \"\"\"Greedy decode function.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\r\n",
        "        model_state: initial state of the model before decoding\r\n",
        "\r\n",
        "    Returns: None\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    delimiter_1 = '1:' \r\n",
        "    delimiter_2 = '2:'\r\n",
        "\r\n",
        "    start_sentence = input('1:')\r\n",
        "    cur_output_tokens = tokenize('1: ' + start_sentence + ' 2:')\r\n",
        "    result = cur_output_tokens.copy()[1:]\r\n",
        "    cur_output = 0\r\n",
        "    stop = 129\r\n",
        "\r\n",
        "    while cur_output != stop:\r\n",
        "\r\n",
        "        # clear past diolog state\r\n",
        "        model.state = model_state\r\n",
        "\r\n",
        "        result.append(cur_output)\r\n",
        "        sentence = detokenize(result)\r\n",
        "\r\n",
        "        if sentence.endswith(delimiter_1):\r\n",
        "            sentence = sentence.split(delimiter_1)[0]\r\n",
        "            print(delimiter_2 + ' ' + sentence)\r\n",
        "            sentence = ''\r\n",
        "            result.clear()\r\n",
        "        \r\n",
        "        # if sentence.endswith(delimiter_2):\r\n",
        "        #     sentence = sentence.split(delimiter_2)[0]\r\n",
        "        #     print(delimiter_1 + ' ' + sentence)\r\n",
        "        #     sentence = ''\r\n",
        "        #     result.clear()\r\n",
        "\r\n",
        "            input_sentence = input('1: ')\r\n",
        "            input_sentence = tokenize(input_sentence)\r\n",
        "            cur_output_tokens += input_sentence + [11]\r\n",
        "            \r\n",
        "\r\n",
        "        cur_output = next_symbol(model, cur_output_tokens)\r\n",
        "        cur_output_tokens.append(cur_output)\r\n",
        "\r\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aedRscFAqI5"
      },
      "source": [
        "interactive_decode_bot_second(ChatModel, STARTING_STATE)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}